{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# Defining column names for datasets\n",
    "COLUMN_NAMES = [\n",
    "        'SepalLength', \n",
    "        'SepalWidth',\n",
    "        'PetalLength', \n",
    "        'PetalWidth', \n",
    "        'Species'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "# Import training dataset\n",
    "training_dataset = pd.read_csv('iris_training.csv', names=COLUMN_NAMES, header=0)\n",
    "train_x = training_dataset.iloc[:, 0:4].values\n",
    "train_y = training_dataset.iloc[:, 4].values\n",
    "\n",
    "# Encoding training dataset\n",
    "encoding_train_y = np_utils.to_categorical(train_y)\n",
    "\n",
    "# Import testing dataset\n",
    "test_dataset = pd.read_csv('iris_test.csv', names=COLUMN_NAMES, header=0)\n",
    "test_x = test_dataset.iloc[:, 0:4].values\n",
    "test_y = test_dataset.iloc[:, 4].values\n",
    "\n",
    "# Encoding training dataset\n",
    "encoding_test_y = np_utils.to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SepalLength  SepalWidth  PetalLength  PetalWidth  Species\n",
       "0           5.9         3.0          4.2         1.5        1\n",
       "1           6.9         3.1          5.4         2.1        2\n",
       "2           5.1         3.3          1.7         0.5        0\n",
       "3           6.0         3.4          4.5         1.6        1\n",
       "4           5.5         2.5          4.0         1.3        1\n",
       "5           6.2         2.9          4.3         1.3        1\n",
       "6           5.5         4.2          1.4         0.2        0\n",
       "7           6.3         2.8          5.1         1.5        2\n",
       "8           5.6         3.0          4.1         1.3        1\n",
       "9           6.7         2.5          5.8         1.8        2\n",
       "10          7.1         3.0          5.9         2.1        2\n",
       "11          4.3         3.0          1.1         0.1        0\n",
       "12          5.6         2.8          4.9         2.0        2\n",
       "13          5.5         2.3          4.0         1.3        1\n",
       "14          6.0         2.2          4.0         1.0        1\n",
       "15          5.1         3.5          1.4         0.2        0\n",
       "16          5.7         2.6          3.5         1.0        1\n",
       "17          4.8         3.4          1.9         0.2        0\n",
       "18          5.1         3.4          1.5         0.2        0\n",
       "19          5.7         2.5          5.0         2.0        2\n",
       "20          5.4         3.4          1.7         0.2        0\n",
       "21          5.6         3.0          4.5         1.5        1\n",
       "22          6.3         2.9          5.6         1.8        2\n",
       "23          6.3         2.5          4.9         1.5        1\n",
       "24          5.8         2.7          3.9         1.2        1\n",
       "25          6.1         3.0          4.6         1.4        1\n",
       "26          5.2         4.1          1.5         0.1        0\n",
       "27          6.7         3.1          4.7         1.5        1\n",
       "28          6.7         3.3          5.7         2.5        2\n",
       "29          6.4         2.9          4.3         1.3        1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "model_1 = Sequential()\n",
    "model_1.add(Dense(10, input_dim=4, activation='relu'))\n",
    "model_1.add(Dense(10, activation='relu'))\n",
    "model_1.add(Dense(3, activation='softmax'))\n",
    "model_1.name=\"model_1\"\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(Dense(10, input_dim=4, activation='relu'))\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "model_2.add(Dense(3, activation='softmax'))\n",
    "model_2.name=\"model_2\"\n",
    "\n",
    "models = [model_1, model_2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, validate_x, train_y, validate_y = train_test_split(train_x, train_y, test_size = 0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 193\n",
      "Trainable params: 193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 10)                50        \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                176       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 549\n",
      "Trainable params: 549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1.1819 - acc: 0.1250 - val_loss: 1.3539 - val_acc: 0.0833\n",
      "Epoch 2/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 1.1730 - acc: 0.0833 - val_loss: 1.3235 - val_acc: 0.0417\n",
      "Epoch 3/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 1.1615 - acc: 0.0729 - val_loss: 1.2941 - val_acc: 0.0833\n",
      "Epoch 4/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 1.1509 - acc: 0.0833 - val_loss: 1.2657 - val_acc: 0.0833\n",
      "Epoch 5/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.1410 - acc: 0.0833 - val_loss: 1.2386 - val_acc: 0.0833\n",
      "Epoch 6/300\n",
      "96/96 [==============================] - 0s 64us/step - loss: 1.1319 - acc: 0.0938 - val_loss: 1.2127 - val_acc: 0.0833\n",
      "Epoch 7/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 1.1236 - acc: 0.1250 - val_loss: 1.1883 - val_acc: 0.0833\n",
      "Epoch 8/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 1.1160 - acc: 0.1146 - val_loss: 1.1653 - val_acc: 0.0833\n",
      "Epoch 9/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 1.1092 - acc: 0.1250 - val_loss: 1.1438 - val_acc: 0.0833\n",
      "Epoch 10/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 1.1031 - acc: 0.1562 - val_loss: 1.1239 - val_acc: 0.1250\n",
      "Epoch 11/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 1.0975 - acc: 0.1562 - val_loss: 1.1055 - val_acc: 0.1250\n",
      "Epoch 12/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 1.0925 - acc: 0.1562 - val_loss: 1.0885 - val_acc: 0.2083\n",
      "Epoch 13/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 1.0880 - acc: 0.1667 - val_loss: 1.0730 - val_acc: 0.2083\n",
      "Epoch 14/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 1.0839 - acc: 0.1875 - val_loss: 1.0589 - val_acc: 0.2083\n",
      "Epoch 15/300\n",
      "96/96 [==============================] - 0s 49us/step - loss: 1.0803 - acc: 0.1979 - val_loss: 1.0462 - val_acc: 0.2917\n",
      "Epoch 16/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 1.0770 - acc: 0.2396 - val_loss: 1.0347 - val_acc: 0.3750\n",
      "Epoch 17/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 1.0740 - acc: 0.2708 - val_loss: 1.0245 - val_acc: 0.4167\n",
      "Epoch 18/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0714 - acc: 0.2917 - val_loss: 1.0154 - val_acc: 0.4583\n",
      "Epoch 19/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 1.0689 - acc: 0.3125 - val_loss: 1.0075 - val_acc: 0.5000\n",
      "Epoch 20/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 1.0666 - acc: 0.3021 - val_loss: 1.0006 - val_acc: 0.5000\n",
      "Epoch 21/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 1.0646 - acc: 0.3333 - val_loss: 0.9946 - val_acc: 0.4583\n",
      "Epoch 22/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 1.0626 - acc: 0.3229 - val_loss: 0.9896 - val_acc: 0.5417\n",
      "Epoch 23/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 1.0608 - acc: 0.3542 - val_loss: 0.9854 - val_acc: 0.6667\n",
      "Epoch 24/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 1.0591 - acc: 0.4271 - val_loss: 0.9820 - val_acc: 0.6667\n",
      "Epoch 25/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 1.0574 - acc: 0.4271 - val_loss: 0.9792 - val_acc: 0.6667\n",
      "Epoch 26/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 1.0559 - acc: 0.4271 - val_loss: 0.9772 - val_acc: 0.6667\n",
      "Epoch 27/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 1.0544 - acc: 0.4271 - val_loss: 0.9757 - val_acc: 0.6667\n",
      "Epoch 28/300\n",
      "96/96 [==============================] - 0s 48us/step - loss: 1.0529 - acc: 0.4375 - val_loss: 0.9748 - val_acc: 0.6667\n",
      "Epoch 29/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 1.0516 - acc: 0.4479 - val_loss: 0.9743 - val_acc: 0.6667\n",
      "Epoch 30/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 1.0503 - acc: 0.4583 - val_loss: 0.9741 - val_acc: 0.6250\n",
      "Epoch 31/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0491 - acc: 0.4583 - val_loss: 0.9743 - val_acc: 0.6250\n",
      "Epoch 32/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 1.0479 - acc: 0.4583 - val_loss: 0.9747 - val_acc: 0.5833\n",
      "Epoch 33/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 1.0468 - acc: 0.4479 - val_loss: 0.9752 - val_acc: 0.5833\n",
      "Epoch 34/300\n",
      "96/96 [==============================] - 0s 45us/step - loss: 1.0458 - acc: 0.4479 - val_loss: 0.9758 - val_acc: 0.5833\n",
      "Epoch 35/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 1.0448 - acc: 0.4583 - val_loss: 0.9764 - val_acc: 0.5833\n",
      "Epoch 36/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 1.0439 - acc: 0.4583 - val_loss: 0.9769 - val_acc: 0.5833\n",
      "Epoch 37/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 1.0430 - acc: 0.4583 - val_loss: 0.9773 - val_acc: 0.5833\n",
      "Epoch 38/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 1.0421 - acc: 0.4583 - val_loss: 0.9775 - val_acc: 0.5833\n",
      "Epoch 39/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 1.0412 - acc: 0.4583 - val_loss: 0.9775 - val_acc: 0.5833\n",
      "Epoch 40/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 1.0403 - acc: 0.4583 - val_loss: 0.9772 - val_acc: 0.5833\n",
      "Epoch 41/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 1.0394 - acc: 0.4583 - val_loss: 0.9767 - val_acc: 0.5417\n",
      "Epoch 42/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 1.0385 - acc: 0.4583 - val_loss: 0.9758 - val_acc: 0.5417\n",
      "Epoch 43/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 1.0375 - acc: 0.4583 - val_loss: 0.9747 - val_acc: 0.5417\n",
      "Epoch 44/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 1.0366 - acc: 0.4583 - val_loss: 0.9734 - val_acc: 0.5417\n",
      "Epoch 45/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0356 - acc: 0.4583 - val_loss: 0.9718 - val_acc: 0.5833\n",
      "Epoch 46/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0346 - acc: 0.4583 - val_loss: 0.9700 - val_acc: 0.5833\n",
      "Epoch 47/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0336 - acc: 0.4583 - val_loss: 0.9681 - val_acc: 0.5833\n",
      "Epoch 48/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0325 - acc: 0.4583 - val_loss: 0.9660 - val_acc: 0.5833\n",
      "Epoch 49/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0315 - acc: 0.4583 - val_loss: 0.9638 - val_acc: 0.5833\n",
      "Epoch 50/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 1.0304 - acc: 0.4583 - val_loss: 0.9616 - val_acc: 0.5833\n",
      "Epoch 51/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 1.0294 - acc: 0.4583 - val_loss: 0.9593 - val_acc: 0.5833\n",
      "Epoch 52/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 1.0283 - acc: 0.4583 - val_loss: 0.9571 - val_acc: 0.5833\n",
      "Epoch 53/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 1.0272 - acc: 0.4583 - val_loss: 0.9550 - val_acc: 0.5833\n",
      "Epoch 54/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 1.0261 - acc: 0.4583 - val_loss: 0.9529 - val_acc: 0.5833\n",
      "Epoch 55/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 1.0251 - acc: 0.4688 - val_loss: 0.9509 - val_acc: 0.5833\n",
      "Epoch 56/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 1.0240 - acc: 0.4688 - val_loss: 0.9491 - val_acc: 0.5833\n",
      "Epoch 57/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 1.0229 - acc: 0.4688 - val_loss: 0.9473 - val_acc: 0.5833\n",
      "Epoch 58/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0218 - acc: 0.4688 - val_loss: 0.9457 - val_acc: 0.5833\n",
      "Epoch 59/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 1.0207 - acc: 0.4688 - val_loss: 0.9443 - val_acc: 0.5833\n",
      "Epoch 60/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 1.0196 - acc: 0.4688 - val_loss: 0.9429 - val_acc: 0.5833\n",
      "Epoch 61/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0185 - acc: 0.4792 - val_loss: 0.9417 - val_acc: 0.5833\n",
      "Epoch 62/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 1.0174 - acc: 0.4792 - val_loss: 0.9406 - val_acc: 0.5833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 1.0163 - acc: 0.4792 - val_loss: 0.9396 - val_acc: 0.5833\n",
      "Epoch 64/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 1.0151 - acc: 0.4792 - val_loss: 0.9387 - val_acc: 0.5833\n",
      "Epoch 65/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 1.0140 - acc: 0.4792 - val_loss: 0.9379 - val_acc: 0.5833\n",
      "Epoch 66/300\n",
      "96/96 [==============================] - 0s 48us/step - loss: 1.0128 - acc: 0.4792 - val_loss: 0.9370 - val_acc: 0.5833\n",
      "Epoch 67/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 1.0117 - acc: 0.4792 - val_loss: 0.9362 - val_acc: 0.5833\n",
      "Epoch 68/300\n",
      "96/96 [==============================] - 0s 61us/step - loss: 1.0105 - acc: 0.4792 - val_loss: 0.9355 - val_acc: 0.5833\n",
      "Epoch 69/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 1.0093 - acc: 0.4688 - val_loss: 0.9346 - val_acc: 0.5833\n",
      "Epoch 70/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0082 - acc: 0.4688 - val_loss: 0.9338 - val_acc: 0.5833\n",
      "Epoch 71/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 1.0070 - acc: 0.4688 - val_loss: 0.9329 - val_acc: 0.5833\n",
      "Epoch 72/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 1.0058 - acc: 0.4688 - val_loss: 0.9319 - val_acc: 0.5833\n",
      "Epoch 73/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 1.0046 - acc: 0.4688 - val_loss: 0.9309 - val_acc: 0.5833\n",
      "Epoch 74/300\n",
      "96/96 [==============================] - 0s 39us/step - loss: 1.0034 - acc: 0.4688 - val_loss: 0.9298 - val_acc: 0.5833\n",
      "Epoch 75/300\n",
      "96/96 [==============================] - 0s 39us/step - loss: 1.0021 - acc: 0.4688 - val_loss: 0.9286 - val_acc: 0.5833\n",
      "Epoch 76/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 1.0009 - acc: 0.4688 - val_loss: 0.9273 - val_acc: 0.5833\n",
      "Epoch 77/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.9997 - acc: 0.4688 - val_loss: 0.9260 - val_acc: 0.5833\n",
      "Epoch 78/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 0.9984 - acc: 0.4688 - val_loss: 0.9245 - val_acc: 0.5833\n",
      "Epoch 79/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.9971 - acc: 0.4688 - val_loss: 0.9230 - val_acc: 0.5833\n",
      "Epoch 80/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9959 - acc: 0.4688 - val_loss: 0.9214 - val_acc: 0.5833\n",
      "Epoch 81/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.9946 - acc: 0.4792 - val_loss: 0.9198 - val_acc: 0.5833\n",
      "Epoch 82/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.9932 - acc: 0.4792 - val_loss: 0.9181 - val_acc: 0.5833\n",
      "Epoch 83/300\n",
      "96/96 [==============================] - 0s 67us/step - loss: 0.9919 - acc: 0.4792 - val_loss: 0.9163 - val_acc: 0.5833\n",
      "Epoch 84/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9906 - acc: 0.4792 - val_loss: 0.9146 - val_acc: 0.5833\n",
      "Epoch 85/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.9892 - acc: 0.4792 - val_loss: 0.9128 - val_acc: 0.5833\n",
      "Epoch 86/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.9879 - acc: 0.4792 - val_loss: 0.9110 - val_acc: 0.5833\n",
      "Epoch 87/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.9865 - acc: 0.4792 - val_loss: 0.9092 - val_acc: 0.5833\n",
      "Epoch 88/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.9851 - acc: 0.4792 - val_loss: 0.9074 - val_acc: 0.5833\n",
      "Epoch 89/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9837 - acc: 0.4792 - val_loss: 0.9056 - val_acc: 0.5833\n",
      "Epoch 90/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.9823 - acc: 0.4792 - val_loss: 0.9039 - val_acc: 0.5833\n",
      "Epoch 91/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.9809 - acc: 0.4792 - val_loss: 0.9021 - val_acc: 0.5833\n",
      "Epoch 92/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9794 - acc: 0.4792 - val_loss: 0.9004 - val_acc: 0.5833\n",
      "Epoch 93/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.9780 - acc: 0.4792 - val_loss: 0.8987 - val_acc: 0.5833\n",
      "Epoch 94/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9765 - acc: 0.4792 - val_loss: 0.8970 - val_acc: 0.5833\n",
      "Epoch 95/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.9750 - acc: 0.4792 - val_loss: 0.8953 - val_acc: 0.6250\n",
      "Epoch 96/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.9735 - acc: 0.4792 - val_loss: 0.8936 - val_acc: 0.6250\n",
      "Epoch 97/300\n",
      "96/96 [==============================] - 0s 17us/step - loss: 0.9720 - acc: 0.4792 - val_loss: 0.8920 - val_acc: 0.6250\n",
      "Epoch 98/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.9705 - acc: 0.4792 - val_loss: 0.8903 - val_acc: 0.6250\n",
      "Epoch 99/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9690 - acc: 0.4792 - val_loss: 0.8886 - val_acc: 0.6250\n",
      "Epoch 100/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.9674 - acc: 0.4792 - val_loss: 0.8870 - val_acc: 0.6250\n",
      "Epoch 101/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.9658 - acc: 0.4792 - val_loss: 0.8853 - val_acc: 0.6250\n",
      "Epoch 102/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.9642 - acc: 0.4792 - val_loss: 0.8836 - val_acc: 0.6250\n",
      "Epoch 103/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.9626 - acc: 0.4792 - val_loss: 0.8819 - val_acc: 0.6250\n",
      "Epoch 104/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.9610 - acc: 0.4792 - val_loss: 0.8801 - val_acc: 0.6250\n",
      "Epoch 105/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.9594 - acc: 0.4792 - val_loss: 0.8784 - val_acc: 0.6250\n",
      "Epoch 106/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.9577 - acc: 0.4792 - val_loss: 0.8766 - val_acc: 0.6250\n",
      "Epoch 107/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.9560 - acc: 0.4792 - val_loss: 0.8748 - val_acc: 0.6250\n",
      "Epoch 108/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.9543 - acc: 0.4792 - val_loss: 0.8729 - val_acc: 0.6250\n",
      "Epoch 109/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.9526 - acc: 0.4792 - val_loss: 0.8711 - val_acc: 0.6250\n",
      "Epoch 110/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.9509 - acc: 0.4792 - val_loss: 0.8692 - val_acc: 0.6250\n",
      "Epoch 111/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.9492 - acc: 0.4792 - val_loss: 0.8672 - val_acc: 0.6250\n",
      "Epoch 112/300\n",
      "96/96 [==============================] - 0s 43us/step - loss: 0.9474 - acc: 0.4792 - val_loss: 0.8653 - val_acc: 0.6250\n",
      "Epoch 113/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.9456 - acc: 0.4792 - val_loss: 0.8633 - val_acc: 0.6250\n",
      "Epoch 114/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.9438 - acc: 0.4792 - val_loss: 0.8613 - val_acc: 0.6250\n",
      "Epoch 115/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.9420 - acc: 0.4792 - val_loss: 0.8593 - val_acc: 0.6250\n",
      "Epoch 116/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.9402 - acc: 0.4792 - val_loss: 0.8573 - val_acc: 0.6250\n",
      "Epoch 117/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9383 - acc: 0.4792 - val_loss: 0.8553 - val_acc: 0.6250\n",
      "Epoch 118/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.9365 - acc: 0.4792 - val_loss: 0.8532 - val_acc: 0.6250\n",
      "Epoch 119/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.9346 - acc: 0.4792 - val_loss: 0.8512 - val_acc: 0.6250\n",
      "Epoch 120/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.9327 - acc: 0.4792 - val_loss: 0.8491 - val_acc: 0.6250\n",
      "Epoch 121/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.9308 - acc: 0.4792 - val_loss: 0.8470 - val_acc: 0.6250\n",
      "Epoch 122/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.9288 - acc: 0.4792 - val_loss: 0.8449 - val_acc: 0.6250\n",
      "Epoch 123/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.9269 - acc: 0.4792 - val_loss: 0.8429 - val_acc: 0.6250\n",
      "Epoch 124/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.9249 - acc: 0.4792 - val_loss: 0.8408 - val_acc: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.9229 - acc: 0.4792 - val_loss: 0.8386 - val_acc: 0.6250\n",
      "Epoch 126/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.9210 - acc: 0.4792 - val_loss: 0.8365 - val_acc: 0.6667\n",
      "Epoch 127/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.9190 - acc: 0.4792 - val_loss: 0.8343 - val_acc: 0.6667\n",
      "Epoch 128/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.9170 - acc: 0.4792 - val_loss: 0.8321 - val_acc: 0.6667\n",
      "Epoch 129/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.9151 - acc: 0.4792 - val_loss: 0.8298 - val_acc: 0.6667\n",
      "Epoch 130/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.9132 - acc: 0.4792 - val_loss: 0.8275 - val_acc: 0.6667\n",
      "Epoch 131/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.9112 - acc: 0.4792 - val_loss: 0.8252 - val_acc: 0.6667\n",
      "Epoch 132/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.9093 - acc: 0.4792 - val_loss: 0.8228 - val_acc: 0.6667\n",
      "Epoch 133/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.9074 - acc: 0.4792 - val_loss: 0.8203 - val_acc: 0.6667\n",
      "Epoch 134/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.9055 - acc: 0.4792 - val_loss: 0.8179 - val_acc: 0.6667\n",
      "Epoch 135/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.9037 - acc: 0.4792 - val_loss: 0.8154 - val_acc: 0.6667\n",
      "Epoch 136/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.9018 - acc: 0.4792 - val_loss: 0.8128 - val_acc: 0.6667\n",
      "Epoch 137/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.8999 - acc: 0.4792 - val_loss: 0.8103 - val_acc: 0.6667\n",
      "Epoch 138/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.8981 - acc: 0.4792 - val_loss: 0.8076 - val_acc: 0.6667\n",
      "Epoch 139/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.8962 - acc: 0.4792 - val_loss: 0.8049 - val_acc: 0.6667\n",
      "Epoch 140/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.8943 - acc: 0.4792 - val_loss: 0.8022 - val_acc: 0.6667\n",
      "Epoch 141/300\n",
      "96/96 [==============================] - 0s 39us/step - loss: 0.8924 - acc: 0.4792 - val_loss: 0.7995 - val_acc: 0.6667\n",
      "Epoch 142/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.8904 - acc: 0.4792 - val_loss: 0.7968 - val_acc: 0.6667\n",
      "Epoch 143/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.8885 - acc: 0.4792 - val_loss: 0.7943 - val_acc: 0.6667\n",
      "Epoch 144/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.8865 - acc: 0.4792 - val_loss: 0.7920 - val_acc: 0.6667\n",
      "Epoch 145/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.8846 - acc: 0.4792 - val_loss: 0.7898 - val_acc: 0.6667\n",
      "Epoch 146/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.8827 - acc: 0.4792 - val_loss: 0.7877 - val_acc: 0.6667\n",
      "Epoch 147/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.8807 - acc: 0.4792 - val_loss: 0.7859 - val_acc: 0.6667\n",
      "Epoch 148/300\n",
      "96/96 [==============================] - 0s 48us/step - loss: 0.8788 - acc: 0.4792 - val_loss: 0.7841 - val_acc: 0.6667\n",
      "Epoch 149/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.8768 - acc: 0.4792 - val_loss: 0.7824 - val_acc: 0.6667\n",
      "Epoch 150/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.8749 - acc: 0.4792 - val_loss: 0.7809 - val_acc: 0.6667\n",
      "Epoch 151/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.8729 - acc: 0.4792 - val_loss: 0.7793 - val_acc: 0.6667\n",
      "Epoch 152/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.8709 - acc: 0.4792 - val_loss: 0.7778 - val_acc: 0.6667\n",
      "Epoch 153/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.8689 - acc: 0.4792 - val_loss: 0.7762 - val_acc: 0.6667\n",
      "Epoch 154/300\n",
      "96/96 [==============================] - 0s 52us/step - loss: 0.8669 - acc: 0.4792 - val_loss: 0.7746 - val_acc: 0.6667\n",
      "Epoch 155/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.8649 - acc: 0.4896 - val_loss: 0.7729 - val_acc: 0.6667\n",
      "Epoch 156/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.8629 - acc: 0.4896 - val_loss: 0.7711 - val_acc: 0.6667\n",
      "Epoch 157/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.8608 - acc: 0.4896 - val_loss: 0.7692 - val_acc: 0.6667\n",
      "Epoch 158/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.8588 - acc: 0.4896 - val_loss: 0.7673 - val_acc: 0.6667\n",
      "Epoch 159/300\n",
      "96/96 [==============================] - 0s 55us/step - loss: 0.8567 - acc: 0.4896 - val_loss: 0.7651 - val_acc: 0.6667\n",
      "Epoch 160/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.8547 - acc: 0.4896 - val_loss: 0.7627 - val_acc: 0.6667\n",
      "Epoch 161/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.8526 - acc: 0.4896 - val_loss: 0.7603 - val_acc: 0.6667\n",
      "Epoch 162/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.8504 - acc: 0.4896 - val_loss: 0.7578 - val_acc: 0.6667\n",
      "Epoch 163/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.8483 - acc: 0.4896 - val_loss: 0.7553 - val_acc: 0.6667\n",
      "Epoch 164/300\n",
      "96/96 [==============================] - 0s 45us/step - loss: 0.8462 - acc: 0.4896 - val_loss: 0.7529 - val_acc: 0.6667\n",
      "Epoch 165/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.8441 - acc: 0.4896 - val_loss: 0.7505 - val_acc: 0.6667\n",
      "Epoch 166/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.8419 - acc: 0.4896 - val_loss: 0.7481 - val_acc: 0.6667\n",
      "Epoch 167/300\n",
      "96/96 [==============================] - 0s 47us/step - loss: 0.8398 - acc: 0.4896 - val_loss: 0.7459 - val_acc: 0.6667\n",
      "Epoch 168/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.8376 - acc: 0.4896 - val_loss: 0.7437 - val_acc: 0.6667\n",
      "Epoch 169/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.8354 - acc: 0.4896 - val_loss: 0.7415 - val_acc: 0.6667\n",
      "Epoch 170/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.8332 - acc: 0.4896 - val_loss: 0.7395 - val_acc: 0.6667\n",
      "Epoch 171/300\n",
      "96/96 [==============================] - 0s 48us/step - loss: 0.8310 - acc: 0.5000 - val_loss: 0.7374 - val_acc: 0.6667\n",
      "Epoch 172/300\n",
      "96/96 [==============================] - 0s 57us/step - loss: 0.8288 - acc: 0.5000 - val_loss: 0.7353 - val_acc: 0.6667\n",
      "Epoch 173/300\n",
      "96/96 [==============================] - 0s 51us/step - loss: 0.8265 - acc: 0.5000 - val_loss: 0.7332 - val_acc: 0.6667\n",
      "Epoch 174/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.8243 - acc: 0.5104 - val_loss: 0.7312 - val_acc: 0.6667\n",
      "Epoch 175/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.8221 - acc: 0.5104 - val_loss: 0.7291 - val_acc: 0.6667\n",
      "Epoch 176/300\n",
      "96/96 [==============================] - 0s 55us/step - loss: 0.8198 - acc: 0.5104 - val_loss: 0.7269 - val_acc: 0.6667\n",
      "Epoch 177/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.8176 - acc: 0.5312 - val_loss: 0.7248 - val_acc: 0.6667\n",
      "Epoch 178/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.8153 - acc: 0.5312 - val_loss: 0.7227 - val_acc: 0.6667\n",
      "Epoch 179/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.8131 - acc: 0.5312 - val_loss: 0.7205 - val_acc: 0.6667\n",
      "Epoch 180/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.8108 - acc: 0.5312 - val_loss: 0.7184 - val_acc: 0.6667\n",
      "Epoch 181/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.8085 - acc: 0.5521 - val_loss: 0.7162 - val_acc: 0.6667\n",
      "Epoch 182/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.8063 - acc: 0.5833 - val_loss: 0.7140 - val_acc: 0.7083\n",
      "Epoch 183/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.8040 - acc: 0.5833 - val_loss: 0.7118 - val_acc: 0.7083\n",
      "Epoch 184/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.8017 - acc: 0.5938 - val_loss: 0.7096 - val_acc: 0.7917\n",
      "Epoch 185/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.7994 - acc: 0.6042 - val_loss: 0.7074 - val_acc: 0.7917\n",
      "Epoch 186/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.7971 - acc: 0.6354 - val_loss: 0.7052 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.7947 - acc: 0.6562 - val_loss: 0.7029 - val_acc: 0.8333\n",
      "Epoch 188/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.7924 - acc: 0.6771 - val_loss: 0.7007 - val_acc: 0.8333\n",
      "Epoch 189/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.7901 - acc: 0.6979 - val_loss: 0.6985 - val_acc: 0.8333\n",
      "Epoch 190/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 0.7877 - acc: 0.7292 - val_loss: 0.6962 - val_acc: 0.9167\n",
      "Epoch 191/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.7854 - acc: 0.7917 - val_loss: 0.6939 - val_acc: 0.9167\n",
      "Epoch 192/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.7830 - acc: 0.8229 - val_loss: 0.6916 - val_acc: 0.9167\n",
      "Epoch 193/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.7807 - acc: 0.8229 - val_loss: 0.6893 - val_acc: 0.9167\n",
      "Epoch 194/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.7783 - acc: 0.8333 - val_loss: 0.6869 - val_acc: 0.9167\n",
      "Epoch 195/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.7760 - acc: 0.8438 - val_loss: 0.6845 - val_acc: 0.9167\n",
      "Epoch 196/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.7736 - acc: 0.8438 - val_loss: 0.6821 - val_acc: 0.9167\n",
      "Epoch 197/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.7712 - acc: 0.8438 - val_loss: 0.6796 - val_acc: 0.9167\n",
      "Epoch 198/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.7688 - acc: 0.8438 - val_loss: 0.6772 - val_acc: 0.9583\n",
      "Epoch 199/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.7665 - acc: 0.8438 - val_loss: 0.6748 - val_acc: 0.9583\n",
      "Epoch 200/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.7641 - acc: 0.8542 - val_loss: 0.6724 - val_acc: 0.9583\n",
      "Epoch 201/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.7617 - acc: 0.8542 - val_loss: 0.6700 - val_acc: 0.9583\n",
      "Epoch 202/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.7593 - acc: 0.8542 - val_loss: 0.6676 - val_acc: 0.9583\n",
      "Epoch 203/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.7569 - acc: 0.8542 - val_loss: 0.6653 - val_acc: 0.9583\n",
      "Epoch 204/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.7545 - acc: 0.8542 - val_loss: 0.6631 - val_acc: 0.9583\n",
      "Epoch 205/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.7521 - acc: 0.8542 - val_loss: 0.6608 - val_acc: 0.9583\n",
      "Epoch 206/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.7497 - acc: 0.8542 - val_loss: 0.6586 - val_acc: 0.9583\n",
      "Epoch 207/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.7472 - acc: 0.8542 - val_loss: 0.6565 - val_acc: 0.9583\n",
      "Epoch 208/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.7448 - acc: 0.8542 - val_loss: 0.6543 - val_acc: 0.9583\n",
      "Epoch 209/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.7424 - acc: 0.8542 - val_loss: 0.6521 - val_acc: 0.9583\n",
      "Epoch 210/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.7400 - acc: 0.8646 - val_loss: 0.6499 - val_acc: 0.9583\n",
      "Epoch 211/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.7375 - acc: 0.8646 - val_loss: 0.6476 - val_acc: 0.9583\n",
      "Epoch 212/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.7351 - acc: 0.8646 - val_loss: 0.6453 - val_acc: 0.9583\n",
      "Epoch 213/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.7327 - acc: 0.8646 - val_loss: 0.6430 - val_acc: 0.9583\n",
      "Epoch 214/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.7302 - acc: 0.8646 - val_loss: 0.6406 - val_acc: 0.9583\n",
      "Epoch 215/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.7278 - acc: 0.8646 - val_loss: 0.6382 - val_acc: 0.9583\n",
      "Epoch 216/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.7253 - acc: 0.8646 - val_loss: 0.6359 - val_acc: 0.9583\n",
      "Epoch 217/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.7229 - acc: 0.8646 - val_loss: 0.6335 - val_acc: 0.9583\n",
      "Epoch 218/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.7204 - acc: 0.8646 - val_loss: 0.6312 - val_acc: 0.9583\n",
      "Epoch 219/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.7179 - acc: 0.8646 - val_loss: 0.6289 - val_acc: 0.9583\n",
      "Epoch 220/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.7155 - acc: 0.8646 - val_loss: 0.6266 - val_acc: 0.9583\n",
      "Epoch 221/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 0.7130 - acc: 0.8646 - val_loss: 0.6244 - val_acc: 0.9583\n",
      "Epoch 222/300\n",
      "96/96 [==============================] - 0s 45us/step - loss: 0.7105 - acc: 0.8646 - val_loss: 0.6221 - val_acc: 0.9583\n",
      "Epoch 223/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.7080 - acc: 0.8646 - val_loss: 0.6199 - val_acc: 0.9583\n",
      "Epoch 224/300\n",
      "96/96 [==============================] - 0s 67us/step - loss: 0.7055 - acc: 0.8646 - val_loss: 0.6177 - val_acc: 0.9583\n",
      "Epoch 225/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.7030 - acc: 0.8646 - val_loss: 0.6155 - val_acc: 0.9583\n",
      "Epoch 226/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.7005 - acc: 0.8646 - val_loss: 0.6134 - val_acc: 0.9583\n",
      "Epoch 227/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 0.6980 - acc: 0.8646 - val_loss: 0.6112 - val_acc: 0.9583\n",
      "Epoch 228/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.6955 - acc: 0.8646 - val_loss: 0.6091 - val_acc: 0.9583\n",
      "Epoch 229/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.6930 - acc: 0.8646 - val_loss: 0.6069 - val_acc: 0.9583\n",
      "Epoch 230/300\n",
      "96/96 [==============================] - 0s 56us/step - loss: 0.6905 - acc: 0.8646 - val_loss: 0.6048 - val_acc: 0.9583\n",
      "Epoch 231/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.6879 - acc: 0.8646 - val_loss: 0.6026 - val_acc: 0.9583\n",
      "Epoch 232/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.6854 - acc: 0.8646 - val_loss: 0.6004 - val_acc: 0.9583\n",
      "Epoch 233/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.6829 - acc: 0.8646 - val_loss: 0.5982 - val_acc: 0.9583\n",
      "Epoch 234/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.6803 - acc: 0.8646 - val_loss: 0.5960 - val_acc: 0.9583\n",
      "Epoch 235/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.6778 - acc: 0.8646 - val_loss: 0.5938 - val_acc: 0.9583\n",
      "Epoch 236/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.6753 - acc: 0.8646 - val_loss: 0.5916 - val_acc: 0.9583\n",
      "Epoch 237/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.6727 - acc: 0.8646 - val_loss: 0.5894 - val_acc: 0.9583\n",
      "Epoch 238/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.6702 - acc: 0.8646 - val_loss: 0.5872 - val_acc: 0.9583\n",
      "Epoch 239/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.6676 - acc: 0.8854 - val_loss: 0.5849 - val_acc: 0.9583\n",
      "Epoch 240/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.6651 - acc: 0.8854 - val_loss: 0.5827 - val_acc: 0.9583\n",
      "Epoch 241/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.6625 - acc: 0.8854 - val_loss: 0.5804 - val_acc: 0.9583\n",
      "Epoch 242/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.6600 - acc: 0.8854 - val_loss: 0.5782 - val_acc: 0.9583\n",
      "Epoch 243/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.6574 - acc: 0.8854 - val_loss: 0.5760 - val_acc: 0.9583\n",
      "Epoch 244/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.6549 - acc: 0.8854 - val_loss: 0.5737 - val_acc: 0.9583\n",
      "Epoch 245/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.6523 - acc: 0.8854 - val_loss: 0.5715 - val_acc: 0.9583\n",
      "Epoch 246/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.6498 - acc: 0.8854 - val_loss: 0.5693 - val_acc: 0.9583\n",
      "Epoch 247/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.6472 - acc: 0.8854 - val_loss: 0.5671 - val_acc: 0.9583\n",
      "Epoch 248/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.6447 - acc: 0.8854 - val_loss: 0.5649 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249/300\n",
      "96/96 [==============================] - 0s 51us/step - loss: 0.6421 - acc: 0.8854 - val_loss: 0.5627 - val_acc: 0.9583\n",
      "Epoch 250/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.6395 - acc: 0.8854 - val_loss: 0.5606 - val_acc: 0.9583\n",
      "Epoch 251/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.6370 - acc: 0.8958 - val_loss: 0.5584 - val_acc: 0.9583\n",
      "Epoch 252/300\n",
      "96/96 [==============================] - 0s 16us/step - loss: 0.6344 - acc: 0.8958 - val_loss: 0.5562 - val_acc: 0.9583\n",
      "Epoch 253/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.6319 - acc: 0.8958 - val_loss: 0.5541 - val_acc: 0.9583\n",
      "Epoch 254/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.6293 - acc: 0.8958 - val_loss: 0.5519 - val_acc: 0.9583\n",
      "Epoch 255/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.6267 - acc: 0.8958 - val_loss: 0.5498 - val_acc: 0.9583\n",
      "Epoch 256/300\n",
      "96/96 [==============================] - 0s 59us/step - loss: 0.6242 - acc: 0.8958 - val_loss: 0.5477 - val_acc: 0.9583\n",
      "Epoch 257/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.6216 - acc: 0.8958 - val_loss: 0.5455 - val_acc: 0.9583\n",
      "Epoch 258/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.6191 - acc: 0.9062 - val_loss: 0.5434 - val_acc: 0.9583\n",
      "Epoch 259/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.6165 - acc: 0.9062 - val_loss: 0.5413 - val_acc: 0.9583\n",
      "Epoch 260/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.6140 - acc: 0.9062 - val_loss: 0.5391 - val_acc: 0.9583\n",
      "Epoch 261/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.6114 - acc: 0.9062 - val_loss: 0.5370 - val_acc: 0.9583\n",
      "Epoch 262/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.6089 - acc: 0.9062 - val_loss: 0.5348 - val_acc: 0.9583\n",
      "Epoch 263/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.6063 - acc: 0.9062 - val_loss: 0.5327 - val_acc: 0.9583\n",
      "Epoch 264/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.6038 - acc: 0.9062 - val_loss: 0.5306 - val_acc: 0.9583\n",
      "Epoch 265/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.6013 - acc: 0.9062 - val_loss: 0.5285 - val_acc: 0.9583\n",
      "Epoch 266/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.5987 - acc: 0.9062 - val_loss: 0.5264 - val_acc: 0.9583\n",
      "Epoch 267/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.5962 - acc: 0.9062 - val_loss: 0.5243 - val_acc: 0.9583\n",
      "Epoch 268/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.5937 - acc: 0.9062 - val_loss: 0.5223 - val_acc: 0.9583\n",
      "Epoch 269/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.5912 - acc: 0.9062 - val_loss: 0.5202 - val_acc: 0.9583\n",
      "Epoch 270/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.5887 - acc: 0.9062 - val_loss: 0.5182 - val_acc: 0.9583\n",
      "Epoch 271/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.5862 - acc: 0.9062 - val_loss: 0.5161 - val_acc: 0.9583\n",
      "Epoch 272/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.5837 - acc: 0.9062 - val_loss: 0.5140 - val_acc: 0.9583\n",
      "Epoch 273/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.5812 - acc: 0.9167 - val_loss: 0.5120 - val_acc: 0.9583\n",
      "Epoch 274/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.5788 - acc: 0.9167 - val_loss: 0.5098 - val_acc: 0.9583\n",
      "Epoch 275/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.5763 - acc: 0.9167 - val_loss: 0.5077 - val_acc: 0.9583\n",
      "Epoch 276/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.5740 - acc: 0.9167 - val_loss: 0.5056 - val_acc: 0.9583\n",
      "Epoch 277/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.5716 - acc: 0.9167 - val_loss: 0.5034 - val_acc: 0.9583\n",
      "Epoch 278/300\n",
      "96/96 [==============================] - 0s 53us/step - loss: 0.5693 - acc: 0.9167 - val_loss: 0.5013 - val_acc: 0.9583\n",
      "Epoch 279/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.5669 - acc: 0.9167 - val_loss: 0.4991 - val_acc: 0.9583\n",
      "Epoch 280/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.5646 - acc: 0.9167 - val_loss: 0.4970 - val_acc: 0.9583\n",
      "Epoch 281/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.5623 - acc: 0.9167 - val_loss: 0.4949 - val_acc: 0.9583\n",
      "Epoch 282/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.5599 - acc: 0.9167 - val_loss: 0.4928 - val_acc: 0.9583\n",
      "Epoch 283/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.5576 - acc: 0.9167 - val_loss: 0.4909 - val_acc: 0.9583\n",
      "Epoch 284/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.5553 - acc: 0.9167 - val_loss: 0.4890 - val_acc: 0.9583\n",
      "Epoch 285/300\n",
      "96/96 [==============================] - 0s 52us/step - loss: 0.5530 - acc: 0.9167 - val_loss: 0.4871 - val_acc: 0.9583\n",
      "Epoch 286/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.5507 - acc: 0.9167 - val_loss: 0.4853 - val_acc: 0.9583\n",
      "Epoch 287/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.5484 - acc: 0.9167 - val_loss: 0.4836 - val_acc: 0.9583\n",
      "Epoch 288/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 0.5461 - acc: 0.9167 - val_loss: 0.4819 - val_acc: 0.9583\n",
      "Epoch 289/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.5439 - acc: 0.9167 - val_loss: 0.4803 - val_acc: 0.9583\n",
      "Epoch 290/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.5416 - acc: 0.9167 - val_loss: 0.4787 - val_acc: 0.9583\n",
      "Epoch 291/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.5393 - acc: 0.9167 - val_loss: 0.4771 - val_acc: 0.9583\n",
      "Epoch 292/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.5371 - acc: 0.9271 - val_loss: 0.4755 - val_acc: 0.9583\n",
      "Epoch 293/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.5348 - acc: 0.9271 - val_loss: 0.4739 - val_acc: 0.9583\n",
      "Epoch 294/300\n",
      "96/96 [==============================] - 0s 18us/step - loss: 0.5326 - acc: 0.9271 - val_loss: 0.4722 - val_acc: 0.9583\n",
      "Epoch 295/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.5304 - acc: 0.9271 - val_loss: 0.4705 - val_acc: 0.9583\n",
      "Epoch 296/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.5281 - acc: 0.9271 - val_loss: 0.4688 - val_acc: 0.9583\n",
      "Epoch 297/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.5259 - acc: 0.9271 - val_loss: 0.4671 - val_acc: 0.9583\n",
      "Epoch 298/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.5237 - acc: 0.9271 - val_loss: 0.4653 - val_acc: 0.9583\n",
      "Epoch 299/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.5215 - acc: 0.9271 - val_loss: 0.4635 - val_acc: 0.9583\n",
      "Epoch 300/300\n",
      "96/96 [==============================] - 0s 61us/step - loss: 0.5193 - acc: 0.9271 - val_loss: 0.4617 - val_acc: 0.9583\n",
      "Train on 96 samples, validate on 24 samples\n",
      "Epoch 1/300\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 1.3064 - acc: 0.3646 - val_loss: 1.1146 - val_acc: 0.2917\n",
      "Epoch 2/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 1.2793 - acc: 0.3646 - val_loss: 1.0895 - val_acc: 0.2917\n",
      "Epoch 3/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 1.2536 - acc: 0.3646 - val_loss: 1.0673 - val_acc: 0.3750\n",
      "Epoch 4/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 1.2295 - acc: 0.3646 - val_loss: 1.0478 - val_acc: 0.4167\n",
      "Epoch 5/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 1.2067 - acc: 0.4062 - val_loss: 1.0300 - val_acc: 0.5417\n",
      "Epoch 6/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 1.1852 - acc: 0.4896 - val_loss: 1.0136 - val_acc: 0.6667\n",
      "Epoch 7/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 1.1649 - acc: 0.5625 - val_loss: 0.9983 - val_acc: 0.7083\n",
      "Epoch 8/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 1.1458 - acc: 0.6458 - val_loss: 0.9848 - val_acc: 0.7917\n",
      "Epoch 9/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 1.1279 - acc: 0.6562 - val_loss: 0.9725 - val_acc: 0.7917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 1.1108 - acc: 0.6562 - val_loss: 0.9592 - val_acc: 0.7917\n",
      "Epoch 11/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 1.0945 - acc: 0.6458 - val_loss: 0.9453 - val_acc: 0.7917\n",
      "Epoch 12/300\n",
      "96/96 [==============================] - 0s 43us/step - loss: 1.0781 - acc: 0.6458 - val_loss: 0.9317 - val_acc: 0.7917\n",
      "Epoch 13/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 1.0620 - acc: 0.6354 - val_loss: 0.9169 - val_acc: 0.7917\n",
      "Epoch 14/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0453 - acc: 0.6354 - val_loss: 0.9015 - val_acc: 0.7917\n",
      "Epoch 15/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 1.0278 - acc: 0.6354 - val_loss: 0.8864 - val_acc: 0.7917\n",
      "Epoch 16/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 1.0099 - acc: 0.6250 - val_loss: 0.8686 - val_acc: 0.7500\n",
      "Epoch 17/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.9921 - acc: 0.6250 - val_loss: 0.8498 - val_acc: 0.7500\n",
      "Epoch 18/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.9766 - acc: 0.6250 - val_loss: 0.8315 - val_acc: 0.6250\n",
      "Epoch 19/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.9632 - acc: 0.3333 - val_loss: 0.8163 - val_acc: 0.5417\n",
      "Epoch 20/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 0.9518 - acc: 0.3021 - val_loss: 0.8052 - val_acc: 0.5417\n",
      "Epoch 21/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.9425 - acc: 0.3125 - val_loss: 0.7964 - val_acc: 0.5417\n",
      "Epoch 22/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.9341 - acc: 0.3229 - val_loss: 0.7897 - val_acc: 0.5417\n",
      "Epoch 23/300\n",
      "96/96 [==============================] - 0s 47us/step - loss: 0.9259 - acc: 0.3229 - val_loss: 0.7849 - val_acc: 0.5417\n",
      "Epoch 24/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.9179 - acc: 0.3438 - val_loss: 0.7817 - val_acc: 0.5417\n",
      "Epoch 25/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.9102 - acc: 0.3542 - val_loss: 0.7796 - val_acc: 0.5833\n",
      "Epoch 26/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.9026 - acc: 0.3854 - val_loss: 0.7785 - val_acc: 0.6250\n",
      "Epoch 27/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.8954 - acc: 0.4479 - val_loss: 0.7780 - val_acc: 0.6667\n",
      "Epoch 28/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.8883 - acc: 0.4479 - val_loss: 0.7778 - val_acc: 0.6667\n",
      "Epoch 29/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.8814 - acc: 0.4583 - val_loss: 0.7774 - val_acc: 0.6667\n",
      "Epoch 30/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.8748 - acc: 0.4896 - val_loss: 0.7766 - val_acc: 0.6667\n",
      "Epoch 31/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.8682 - acc: 0.4896 - val_loss: 0.7749 - val_acc: 0.6667\n",
      "Epoch 32/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.8618 - acc: 0.5104 - val_loss: 0.7722 - val_acc: 0.6667\n",
      "Epoch 33/300\n",
      "96/96 [==============================] - 0s 45us/step - loss: 0.8553 - acc: 0.5104 - val_loss: 0.7684 - val_acc: 0.6250\n",
      "Epoch 34/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.8489 - acc: 0.5521 - val_loss: 0.7633 - val_acc: 0.6250\n",
      "Epoch 35/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.8423 - acc: 0.5521 - val_loss: 0.7569 - val_acc: 0.6250\n",
      "Epoch 36/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.8357 - acc: 0.5521 - val_loss: 0.7495 - val_acc: 0.6250\n",
      "Epoch 37/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.8289 - acc: 0.5521 - val_loss: 0.7414 - val_acc: 0.6667\n",
      "Epoch 38/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.8222 - acc: 0.5521 - val_loss: 0.7327 - val_acc: 0.6667\n",
      "Epoch 39/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.8155 - acc: 0.5521 - val_loss: 0.7236 - val_acc: 0.6667\n",
      "Epoch 40/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.8088 - acc: 0.5312 - val_loss: 0.7147 - val_acc: 0.6667\n",
      "Epoch 41/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.8022 - acc: 0.5208 - val_loss: 0.7059 - val_acc: 0.6667\n",
      "Epoch 42/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.7956 - acc: 0.5104 - val_loss: 0.6975 - val_acc: 0.6667\n",
      "Epoch 43/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.7890 - acc: 0.5104 - val_loss: 0.6894 - val_acc: 0.6667\n",
      "Epoch 44/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.7824 - acc: 0.5104 - val_loss: 0.6818 - val_acc: 0.6667\n",
      "Epoch 45/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.7758 - acc: 0.5104 - val_loss: 0.6746 - val_acc: 0.6667\n",
      "Epoch 46/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.7691 - acc: 0.5104 - val_loss: 0.6679 - val_acc: 0.6667\n",
      "Epoch 47/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.7623 - acc: 0.5521 - val_loss: 0.6615 - val_acc: 0.9583\n",
      "Epoch 48/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.7556 - acc: 0.8438 - val_loss: 0.6556 - val_acc: 0.9583\n",
      "Epoch 49/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 0.7488 - acc: 0.8646 - val_loss: 0.6500 - val_acc: 0.9583\n",
      "Epoch 50/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.7420 - acc: 0.8750 - val_loss: 0.6446 - val_acc: 0.9583\n",
      "Epoch 51/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.7352 - acc: 0.8854 - val_loss: 0.6394 - val_acc: 0.9583\n",
      "Epoch 52/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.7285 - acc: 0.8854 - val_loss: 0.6342 - val_acc: 0.9583\n",
      "Epoch 53/300\n",
      "96/96 [==============================] - 0s 48us/step - loss: 0.7217 - acc: 0.8958 - val_loss: 0.6289 - val_acc: 0.9583\n",
      "Epoch 54/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.7150 - acc: 0.9062 - val_loss: 0.6236 - val_acc: 0.9583\n",
      "Epoch 55/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.7083 - acc: 0.9062 - val_loss: 0.6181 - val_acc: 0.9583\n",
      "Epoch 56/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.7016 - acc: 0.9062 - val_loss: 0.6123 - val_acc: 0.9583\n",
      "Epoch 57/300\n",
      "96/96 [==============================] - 0s 55us/step - loss: 0.6949 - acc: 0.9062 - val_loss: 0.6063 - val_acc: 0.9583\n",
      "Epoch 58/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.6883 - acc: 0.9167 - val_loss: 0.6000 - val_acc: 0.9583\n",
      "Epoch 59/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.6818 - acc: 0.9167 - val_loss: 0.5935 - val_acc: 0.9583\n",
      "Epoch 60/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.6752 - acc: 0.9167 - val_loss: 0.5868 - val_acc: 0.9583\n",
      "Epoch 61/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.6686 - acc: 0.9167 - val_loss: 0.5800 - val_acc: 0.9583\n",
      "Epoch 62/300\n",
      "96/96 [==============================] - 0s 49us/step - loss: 0.6621 - acc: 0.9062 - val_loss: 0.5733 - val_acc: 0.9583\n",
      "Epoch 63/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 0.6556 - acc: 0.9062 - val_loss: 0.5667 - val_acc: 0.9583\n",
      "Epoch 64/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.6491 - acc: 0.9062 - val_loss: 0.5603 - val_acc: 0.9583\n",
      "Epoch 65/300\n",
      "96/96 [==============================] - 0s 17us/step - loss: 0.6426 - acc: 0.9062 - val_loss: 0.5542 - val_acc: 0.9583\n",
      "Epoch 66/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.6362 - acc: 0.9062 - val_loss: 0.5484 - val_acc: 0.9583\n",
      "Epoch 67/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.6298 - acc: 0.9062 - val_loss: 0.5429 - val_acc: 0.9583\n",
      "Epoch 68/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.6234 - acc: 0.9167 - val_loss: 0.5378 - val_acc: 0.9583\n",
      "Epoch 69/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.6170 - acc: 0.9167 - val_loss: 0.5329 - val_acc: 0.9583\n",
      "Epoch 70/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.6106 - acc: 0.9167 - val_loss: 0.5282 - val_acc: 0.9583\n",
      "Epoch 71/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.6043 - acc: 0.9167 - val_loss: 0.5237 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.5981 - acc: 0.9271 - val_loss: 0.5193 - val_acc: 0.9583\n",
      "Epoch 73/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.5919 - acc: 0.9271 - val_loss: 0.5150 - val_acc: 0.9583\n",
      "Epoch 74/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.5858 - acc: 0.9271 - val_loss: 0.5106 - val_acc: 0.9583\n",
      "Epoch 75/300\n",
      "96/96 [==============================] - 0s 18us/step - loss: 0.5796 - acc: 0.9375 - val_loss: 0.5060 - val_acc: 0.9583\n",
      "Epoch 76/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.5735 - acc: 0.9375 - val_loss: 0.5013 - val_acc: 0.9583\n",
      "Epoch 77/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.5674 - acc: 0.9375 - val_loss: 0.4962 - val_acc: 0.9583\n",
      "Epoch 78/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.5613 - acc: 0.9375 - val_loss: 0.4910 - val_acc: 0.9583\n",
      "Epoch 79/300\n",
      "96/96 [==============================] - 0s 53us/step - loss: 0.5553 - acc: 0.9375 - val_loss: 0.4854 - val_acc: 0.9583\n",
      "Epoch 80/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.5493 - acc: 0.9479 - val_loss: 0.4797 - val_acc: 0.9583\n",
      "Epoch 81/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.5434 - acc: 0.9479 - val_loss: 0.4740 - val_acc: 0.9583\n",
      "Epoch 82/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.5376 - acc: 0.9479 - val_loss: 0.4683 - val_acc: 0.9583\n",
      "Epoch 83/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.5318 - acc: 0.9479 - val_loss: 0.4628 - val_acc: 0.9583\n",
      "Epoch 84/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.5261 - acc: 0.9479 - val_loss: 0.4577 - val_acc: 0.9583\n",
      "Epoch 85/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.5205 - acc: 0.9479 - val_loss: 0.4529 - val_acc: 0.9583\n",
      "Epoch 86/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.5150 - acc: 0.9479 - val_loss: 0.4483 - val_acc: 0.9583\n",
      "Epoch 87/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.5096 - acc: 0.9479 - val_loss: 0.4438 - val_acc: 0.9583\n",
      "Epoch 88/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.5043 - acc: 0.9479 - val_loss: 0.4395 - val_acc: 0.9583\n",
      "Epoch 89/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.4989 - acc: 0.9479 - val_loss: 0.4353 - val_acc: 0.9583\n",
      "Epoch 90/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.4937 - acc: 0.9479 - val_loss: 0.4311 - val_acc: 0.9583\n",
      "Epoch 91/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.4884 - acc: 0.9583 - val_loss: 0.4270 - val_acc: 0.9583\n",
      "Epoch 92/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.4833 - acc: 0.9583 - val_loss: 0.4229 - val_acc: 0.9583\n",
      "Epoch 93/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.4781 - acc: 0.9583 - val_loss: 0.4187 - val_acc: 0.9583\n",
      "Epoch 94/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.4731 - acc: 0.9583 - val_loss: 0.4144 - val_acc: 0.9583\n",
      "Epoch 95/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.4680 - acc: 0.9583 - val_loss: 0.4102 - val_acc: 0.9583\n",
      "Epoch 96/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.4630 - acc: 0.9688 - val_loss: 0.4059 - val_acc: 0.9583\n",
      "Epoch 97/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.4580 - acc: 0.9688 - val_loss: 0.4018 - val_acc: 0.9583\n",
      "Epoch 98/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.4531 - acc: 0.9688 - val_loss: 0.3978 - val_acc: 0.9583\n",
      "Epoch 99/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.4483 - acc: 0.9792 - val_loss: 0.3940 - val_acc: 0.9583\n",
      "Epoch 100/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.4435 - acc: 0.9792 - val_loss: 0.3904 - val_acc: 0.9583\n",
      "Epoch 101/300\n",
      "96/96 [==============================] - 0s 53us/step - loss: 0.4387 - acc: 0.9792 - val_loss: 0.3869 - val_acc: 0.9583\n",
      "Epoch 102/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.4340 - acc: 0.9792 - val_loss: 0.3836 - val_acc: 0.9583\n",
      "Epoch 103/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.4294 - acc: 0.9792 - val_loss: 0.3803 - val_acc: 0.9583\n",
      "Epoch 104/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.4249 - acc: 0.9792 - val_loss: 0.3772 - val_acc: 0.9167\n",
      "Epoch 105/300\n",
      "96/96 [==============================] - 0s 39us/step - loss: 0.4204 - acc: 0.9792 - val_loss: 0.3743 - val_acc: 0.9167\n",
      "Epoch 106/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.4159 - acc: 0.9792 - val_loss: 0.3713 - val_acc: 0.9167\n",
      "Epoch 107/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 0.4115 - acc: 0.9792 - val_loss: 0.3683 - val_acc: 0.9167\n",
      "Epoch 108/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.4071 - acc: 0.9792 - val_loss: 0.3653 - val_acc: 0.9167\n",
      "Epoch 109/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.4028 - acc: 0.9792 - val_loss: 0.3623 - val_acc: 0.9167\n",
      "Epoch 110/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.3986 - acc: 0.9792 - val_loss: 0.3592 - val_acc: 0.9167\n",
      "Epoch 111/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.3943 - acc: 0.9792 - val_loss: 0.3562 - val_acc: 0.9167\n",
      "Epoch 112/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.3902 - acc: 0.9792 - val_loss: 0.3531 - val_acc: 0.9167\n",
      "Epoch 113/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.3860 - acc: 0.9792 - val_loss: 0.3500 - val_acc: 0.9167\n",
      "Epoch 114/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.3820 - acc: 0.9792 - val_loss: 0.3469 - val_acc: 0.9167\n",
      "Epoch 115/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 0.3779 - acc: 0.9792 - val_loss: 0.3439 - val_acc: 0.9167\n",
      "Epoch 116/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.3739 - acc: 0.9792 - val_loss: 0.3409 - val_acc: 0.9167\n",
      "Epoch 117/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.3700 - acc: 0.9792 - val_loss: 0.3379 - val_acc: 0.9167\n",
      "Epoch 118/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.3661 - acc: 0.9792 - val_loss: 0.3349 - val_acc: 0.9167\n",
      "Epoch 119/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.3622 - acc: 0.9792 - val_loss: 0.3320 - val_acc: 0.9167\n",
      "Epoch 120/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.3584 - acc: 0.9792 - val_loss: 0.3290 - val_acc: 0.9167\n",
      "Epoch 121/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.3546 - acc: 0.9792 - val_loss: 0.3260 - val_acc: 0.9167\n",
      "Epoch 122/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.3508 - acc: 0.9792 - val_loss: 0.3231 - val_acc: 0.9167\n",
      "Epoch 123/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.3470 - acc: 0.9792 - val_loss: 0.3201 - val_acc: 0.9167\n",
      "Epoch 124/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.3432 - acc: 0.9792 - val_loss: 0.3172 - val_acc: 0.9167\n",
      "Epoch 125/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.3394 - acc: 0.9792 - val_loss: 0.3142 - val_acc: 0.9167\n",
      "Epoch 126/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.3355 - acc: 0.9792 - val_loss: 0.3112 - val_acc: 0.9167\n",
      "Epoch 127/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.3317 - acc: 0.9792 - val_loss: 0.3081 - val_acc: 0.9167\n",
      "Epoch 128/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.3279 - acc: 0.9792 - val_loss: 0.3052 - val_acc: 0.9167\n",
      "Epoch 129/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.3242 - acc: 0.9792 - val_loss: 0.3023 - val_acc: 0.9167\n",
      "Epoch 130/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.3207 - acc: 0.9792 - val_loss: 0.3001 - val_acc: 0.9167\n",
      "Epoch 131/300\n",
      "96/96 [==============================] - 0s 49us/step - loss: 0.3174 - acc: 0.9792 - val_loss: 0.2984 - val_acc: 0.9167\n",
      "Epoch 132/300\n",
      "96/96 [==============================] - 0s 53us/step - loss: 0.3140 - acc: 0.9792 - val_loss: 0.2969 - val_acc: 0.9167\n",
      "Epoch 133/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.3106 - acc: 0.9792 - val_loss: 0.2956 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.3071 - acc: 0.9792 - val_loss: 0.2939 - val_acc: 0.9167\n",
      "Epoch 135/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.3037 - acc: 0.9792 - val_loss: 0.2919 - val_acc: 0.9167\n",
      "Epoch 136/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.3003 - acc: 0.9792 - val_loss: 0.2893 - val_acc: 0.9167\n",
      "Epoch 137/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.2970 - acc: 0.9792 - val_loss: 0.2863 - val_acc: 0.9167\n",
      "Epoch 138/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.2937 - acc: 0.9792 - val_loss: 0.2829 - val_acc: 0.9167\n",
      "Epoch 139/300\n",
      "96/96 [==============================] - 0s 63us/step - loss: 0.2905 - acc: 0.9792 - val_loss: 0.2794 - val_acc: 0.9167\n",
      "Epoch 140/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.2873 - acc: 0.9792 - val_loss: 0.2760 - val_acc: 0.9167\n",
      "Epoch 141/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.2842 - acc: 0.9792 - val_loss: 0.2729 - val_acc: 0.9167\n",
      "Epoch 142/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.2811 - acc: 0.9792 - val_loss: 0.2703 - val_acc: 0.9167\n",
      "Epoch 143/300\n",
      "96/96 [==============================] - 0s 47us/step - loss: 0.2780 - acc: 0.9792 - val_loss: 0.2683 - val_acc: 0.9167\n",
      "Epoch 144/300\n",
      "96/96 [==============================] - 0s 39us/step - loss: 0.2749 - acc: 0.9792 - val_loss: 0.2666 - val_acc: 0.9167\n",
      "Epoch 145/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.2718 - acc: 0.9792 - val_loss: 0.2652 - val_acc: 0.9167\n",
      "Epoch 146/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.2688 - acc: 0.9792 - val_loss: 0.2640 - val_acc: 0.9167\n",
      "Epoch 147/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.2658 - acc: 0.9792 - val_loss: 0.2626 - val_acc: 0.9167\n",
      "Epoch 148/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.2629 - acc: 0.9792 - val_loss: 0.2608 - val_acc: 0.9167\n",
      "Epoch 149/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.2600 - acc: 0.9792 - val_loss: 0.2586 - val_acc: 0.9167\n",
      "Epoch 150/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.2571 - acc: 0.9792 - val_loss: 0.2561 - val_acc: 0.9167\n",
      "Epoch 151/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.2542 - acc: 0.9792 - val_loss: 0.2532 - val_acc: 0.9167\n",
      "Epoch 152/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.2514 - acc: 0.9792 - val_loss: 0.2503 - val_acc: 0.9167\n",
      "Epoch 153/300\n",
      "96/96 [==============================] - 0s 52us/step - loss: 0.2486 - acc: 0.9792 - val_loss: 0.2475 - val_acc: 0.9167\n",
      "Epoch 154/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.2459 - acc: 0.9792 - val_loss: 0.2451 - val_acc: 0.9167\n",
      "Epoch 155/300\n",
      "96/96 [==============================] - 0s 41us/step - loss: 0.2431 - acc: 0.9792 - val_loss: 0.2431 - val_acc: 0.9167\n",
      "Epoch 156/300\n",
      "96/96 [==============================] - 0s 56us/step - loss: 0.2404 - acc: 0.9792 - val_loss: 0.2414 - val_acc: 0.9167\n",
      "Epoch 157/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.2377 - acc: 0.9792 - val_loss: 0.2401 - val_acc: 0.9167\n",
      "Epoch 158/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.2351 - acc: 0.9792 - val_loss: 0.2388 - val_acc: 0.9167\n",
      "Epoch 159/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.2325 - acc: 0.9792 - val_loss: 0.2373 - val_acc: 0.9167\n",
      "Epoch 160/300\n",
      "96/96 [==============================] - 0s 39us/step - loss: 0.2299 - acc: 0.9792 - val_loss: 0.2355 - val_acc: 0.9167\n",
      "Epoch 161/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.2273 - acc: 0.9792 - val_loss: 0.2335 - val_acc: 0.9167\n",
      "Epoch 162/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.2248 - acc: 0.9792 - val_loss: 0.2311 - val_acc: 0.9167\n",
      "Epoch 163/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.2223 - acc: 0.9792 - val_loss: 0.2288 - val_acc: 0.9167\n",
      "Epoch 164/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.2198 - acc: 0.9792 - val_loss: 0.2265 - val_acc: 0.9167\n",
      "Epoch 165/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.2174 - acc: 0.9792 - val_loss: 0.2244 - val_acc: 0.9167\n",
      "Epoch 166/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.2150 - acc: 0.9792 - val_loss: 0.2226 - val_acc: 0.9167\n",
      "Epoch 167/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.2126 - acc: 0.9792 - val_loss: 0.2210 - val_acc: 0.9167\n",
      "Epoch 168/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.2103 - acc: 0.9792 - val_loss: 0.2197 - val_acc: 0.9167\n",
      "Epoch 169/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.2079 - acc: 0.9792 - val_loss: 0.2184 - val_acc: 0.9167\n",
      "Epoch 170/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.2057 - acc: 0.9792 - val_loss: 0.2171 - val_acc: 0.9167\n",
      "Epoch 171/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.2034 - acc: 0.9792 - val_loss: 0.2156 - val_acc: 0.9167\n",
      "Epoch 172/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.2012 - acc: 0.9792 - val_loss: 0.2139 - val_acc: 0.9167\n",
      "Epoch 173/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.1990 - acc: 0.9792 - val_loss: 0.2121 - val_acc: 0.9167\n",
      "Epoch 174/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.1968 - acc: 0.9792 - val_loss: 0.2101 - val_acc: 0.9167\n",
      "Epoch 175/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.1946 - acc: 0.9792 - val_loss: 0.2083 - val_acc: 0.9167\n",
      "Epoch 176/300\n",
      "96/96 [==============================] - 0s 40us/step - loss: 0.1925 - acc: 0.9792 - val_loss: 0.2065 - val_acc: 0.9167\n",
      "Epoch 177/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.1904 - acc: 0.9792 - val_loss: 0.2050 - val_acc: 0.9167\n",
      "Epoch 178/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.1884 - acc: 0.9792 - val_loss: 0.2036 - val_acc: 0.9167\n",
      "Epoch 179/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.1863 - acc: 0.9792 - val_loss: 0.2021 - val_acc: 0.9167\n",
      "Epoch 180/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1843 - acc: 0.9792 - val_loss: 0.2008 - val_acc: 0.9167\n",
      "Epoch 181/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.1823 - acc: 0.9792 - val_loss: 0.1994 - val_acc: 0.9167\n",
      "Epoch 182/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.1804 - acc: 0.9792 - val_loss: 0.1980 - val_acc: 0.9167\n",
      "Epoch 183/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.1785 - acc: 0.9792 - val_loss: 0.1964 - val_acc: 0.9167\n",
      "Epoch 184/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.1766 - acc: 0.9792 - val_loss: 0.1949 - val_acc: 0.9167\n",
      "Epoch 185/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.1747 - acc: 0.9792 - val_loss: 0.1933 - val_acc: 0.9167\n",
      "Epoch 186/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.1729 - acc: 0.9792 - val_loss: 0.1918 - val_acc: 0.9167\n",
      "Epoch 187/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.1710 - acc: 0.9792 - val_loss: 0.1904 - val_acc: 0.9167\n",
      "Epoch 188/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.1693 - acc: 0.9792 - val_loss: 0.1891 - val_acc: 0.9167\n",
      "Epoch 189/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.1675 - acc: 0.9792 - val_loss: 0.1880 - val_acc: 0.9167\n",
      "Epoch 190/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.1658 - acc: 0.9792 - val_loss: 0.1869 - val_acc: 0.9167\n",
      "Epoch 191/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.1641 - acc: 0.9792 - val_loss: 0.1859 - val_acc: 0.9167\n",
      "Epoch 192/300\n",
      "96/96 [==============================] - 0s 49us/step - loss: 0.1624 - acc: 0.9792 - val_loss: 0.1849 - val_acc: 0.9167\n",
      "Epoch 193/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.1607 - acc: 0.9792 - val_loss: 0.1837 - val_acc: 0.9167\n",
      "Epoch 194/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1591 - acc: 0.9792 - val_loss: 0.1824 - val_acc: 0.9167\n",
      "Epoch 195/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.1574 - acc: 0.9792 - val_loss: 0.1810 - val_acc: 0.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.1559 - acc: 0.9792 - val_loss: 0.1796 - val_acc: 0.9167\n",
      "Epoch 197/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.1543 - acc: 0.9792 - val_loss: 0.1783 - val_acc: 0.9167\n",
      "Epoch 198/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1527 - acc: 0.9792 - val_loss: 0.1771 - val_acc: 0.9167\n",
      "Epoch 199/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1512 - acc: 0.9792 - val_loss: 0.1760 - val_acc: 0.9167\n",
      "Epoch 200/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.1497 - acc: 0.9792 - val_loss: 0.1749 - val_acc: 0.9167\n",
      "Epoch 201/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.1482 - acc: 0.9792 - val_loss: 0.1738 - val_acc: 0.9167\n",
      "Epoch 202/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.1467 - acc: 0.9792 - val_loss: 0.1728 - val_acc: 0.9167\n",
      "Epoch 203/300\n",
      "96/96 [==============================] - 0s 21us/step - loss: 0.1453 - acc: 0.9792 - val_loss: 0.1717 - val_acc: 0.9167\n",
      "Epoch 204/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.1439 - acc: 0.9792 - val_loss: 0.1707 - val_acc: 0.9167\n",
      "Epoch 205/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.1425 - acc: 0.9792 - val_loss: 0.1697 - val_acc: 0.9167\n",
      "Epoch 206/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.1411 - acc: 0.9792 - val_loss: 0.1685 - val_acc: 0.9167\n",
      "Epoch 207/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.1398 - acc: 0.9792 - val_loss: 0.1674 - val_acc: 0.9167\n",
      "Epoch 208/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.1384 - acc: 0.9792 - val_loss: 0.1664 - val_acc: 0.9167\n",
      "Epoch 209/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.1371 - acc: 0.9792 - val_loss: 0.1654 - val_acc: 0.9167\n",
      "Epoch 210/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.1358 - acc: 0.9792 - val_loss: 0.1643 - val_acc: 0.9167\n",
      "Epoch 211/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.1346 - acc: 0.9792 - val_loss: 0.1633 - val_acc: 0.9167\n",
      "Epoch 212/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.1333 - acc: 0.9792 - val_loss: 0.1624 - val_acc: 0.9167\n",
      "Epoch 213/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.1321 - acc: 0.9792 - val_loss: 0.1614 - val_acc: 0.9167\n",
      "Epoch 214/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1309 - acc: 0.9792 - val_loss: 0.1605 - val_acc: 0.9167\n",
      "Epoch 215/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.1297 - acc: 0.9792 - val_loss: 0.1597 - val_acc: 0.9167\n",
      "Epoch 216/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.1286 - acc: 0.9792 - val_loss: 0.1587 - val_acc: 0.9167\n",
      "Epoch 217/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.1274 - acc: 0.9792 - val_loss: 0.1577 - val_acc: 0.9167\n",
      "Epoch 218/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.1263 - acc: 0.9792 - val_loss: 0.1568 - val_acc: 0.9167\n",
      "Epoch 219/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.1252 - acc: 0.9792 - val_loss: 0.1560 - val_acc: 0.9167\n",
      "Epoch 220/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.1241 - acc: 0.9792 - val_loss: 0.1552 - val_acc: 0.9167\n",
      "Epoch 221/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.1230 - acc: 0.9792 - val_loss: 0.1544 - val_acc: 0.9167\n",
      "Epoch 222/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.1220 - acc: 0.9792 - val_loss: 0.1537 - val_acc: 0.9167\n",
      "Epoch 223/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.1210 - acc: 0.9792 - val_loss: 0.1529 - val_acc: 0.9167\n",
      "Epoch 224/300\n",
      "96/96 [==============================] - 0s 62us/step - loss: 0.1200 - acc: 0.9792 - val_loss: 0.1521 - val_acc: 0.9167\n",
      "Epoch 225/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.1190 - acc: 0.9792 - val_loss: 0.1513 - val_acc: 0.9167\n",
      "Epoch 226/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.1180 - acc: 0.9792 - val_loss: 0.1504 - val_acc: 0.9167\n",
      "Epoch 227/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.1171 - acc: 0.9792 - val_loss: 0.1496 - val_acc: 0.9167\n",
      "Epoch 228/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.1161 - acc: 0.9792 - val_loss: 0.1488 - val_acc: 0.9167\n",
      "Epoch 229/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.1152 - acc: 0.9792 - val_loss: 0.1480 - val_acc: 0.9167\n",
      "Epoch 230/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1143 - acc: 0.9792 - val_loss: 0.1472 - val_acc: 0.9167\n",
      "Epoch 231/300\n",
      "96/96 [==============================] - 0s 20us/step - loss: 0.1134 - acc: 0.9792 - val_loss: 0.1465 - val_acc: 0.9167\n",
      "Epoch 232/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.1126 - acc: 0.9792 - val_loss: 0.1457 - val_acc: 0.9167\n",
      "Epoch 233/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.1117 - acc: 0.9792 - val_loss: 0.1450 - val_acc: 0.9167\n",
      "Epoch 234/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.1109 - acc: 0.9792 - val_loss: 0.1443 - val_acc: 0.9167\n",
      "Epoch 235/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.1100 - acc: 0.9792 - val_loss: 0.1436 - val_acc: 0.9167\n",
      "Epoch 236/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.1092 - acc: 0.9792 - val_loss: 0.1429 - val_acc: 0.9167\n",
      "Epoch 237/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.1084 - acc: 0.9792 - val_loss: 0.1423 - val_acc: 0.9167\n",
      "Epoch 238/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.1076 - acc: 0.9792 - val_loss: 0.1416 - val_acc: 0.9167\n",
      "Epoch 239/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.1069 - acc: 0.9792 - val_loss: 0.1409 - val_acc: 0.9167\n",
      "Epoch 240/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.1061 - acc: 0.9792 - val_loss: 0.1403 - val_acc: 0.9167\n",
      "Epoch 241/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.1054 - acc: 0.9792 - val_loss: 0.1398 - val_acc: 0.9583\n",
      "Epoch 242/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1046 - acc: 0.9792 - val_loss: 0.1392 - val_acc: 0.9583\n",
      "Epoch 243/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.1039 - acc: 0.9792 - val_loss: 0.1386 - val_acc: 0.9583\n",
      "Epoch 244/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.1032 - acc: 0.9792 - val_loss: 0.1380 - val_acc: 0.9583\n",
      "Epoch 245/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.1025 - acc: 0.9792 - val_loss: 0.1374 - val_acc: 0.9583\n",
      "Epoch 246/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.1018 - acc: 0.9792 - val_loss: 0.1368 - val_acc: 0.9583\n",
      "Epoch 247/300\n",
      "96/96 [==============================] - 0s 22us/step - loss: 0.1011 - acc: 0.9792 - val_loss: 0.1362 - val_acc: 0.9583\n",
      "Epoch 248/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.1005 - acc: 0.9792 - val_loss: 0.1357 - val_acc: 0.9583\n",
      "Epoch 249/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.0998 - acc: 0.9792 - val_loss: 0.1352 - val_acc: 0.9583\n",
      "Epoch 250/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.0992 - acc: 0.9792 - val_loss: 0.1347 - val_acc: 0.9583\n",
      "Epoch 251/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.0985 - acc: 0.9792 - val_loss: 0.1342 - val_acc: 0.9583\n",
      "Epoch 252/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.0979 - acc: 0.9792 - val_loss: 0.1336 - val_acc: 0.9583\n",
      "Epoch 253/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.0973 - acc: 0.9792 - val_loss: 0.1331 - val_acc: 0.9583\n",
      "Epoch 254/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 0.0967 - acc: 0.9792 - val_loss: 0.1326 - val_acc: 0.9583\n",
      "Epoch 255/300\n",
      "96/96 [==============================] - 0s 27us/step - loss: 0.0961 - acc: 0.9792 - val_loss: 0.1321 - val_acc: 0.9583\n",
      "Epoch 256/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.0955 - acc: 0.9792 - val_loss: 0.1316 - val_acc: 0.9583\n",
      "Epoch 257/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.0949 - acc: 0.9792 - val_loss: 0.1311 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258/300\n",
      "96/96 [==============================] - 0s 43us/step - loss: 0.0943 - acc: 0.9792 - val_loss: 0.1306 - val_acc: 0.9583\n",
      "Epoch 259/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.0938 - acc: 0.9792 - val_loss: 0.1301 - val_acc: 0.9583\n",
      "Epoch 260/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.0932 - acc: 0.9792 - val_loss: 0.1296 - val_acc: 0.9583\n",
      "Epoch 261/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.0927 - acc: 0.9792 - val_loss: 0.1292 - val_acc: 0.9583\n",
      "Epoch 262/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.0922 - acc: 0.9792 - val_loss: 0.1287 - val_acc: 0.9583\n",
      "Epoch 263/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.0916 - acc: 0.9792 - val_loss: 0.1282 - val_acc: 0.9583\n",
      "Epoch 264/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.0911 - acc: 0.9792 - val_loss: 0.1278 - val_acc: 0.9583\n",
      "Epoch 265/300\n",
      "96/96 [==============================] - 0s 49us/step - loss: 0.0906 - acc: 0.9792 - val_loss: 0.1273 - val_acc: 0.9583\n",
      "Epoch 266/300\n",
      "96/96 [==============================] - 0s 66us/step - loss: 0.0901 - acc: 0.9792 - val_loss: 0.1269 - val_acc: 0.9583\n",
      "Epoch 267/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.0896 - acc: 0.9792 - val_loss: 0.1265 - val_acc: 0.9583\n",
      "Epoch 268/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.0891 - acc: 0.9792 - val_loss: 0.1261 - val_acc: 0.9583\n",
      "Epoch 269/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 0.0887 - acc: 0.9792 - val_loss: 0.1257 - val_acc: 0.9583\n",
      "Epoch 270/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.0882 - acc: 0.9792 - val_loss: 0.1253 - val_acc: 0.9583\n",
      "Epoch 271/300\n",
      "96/96 [==============================] - 0s 37us/step - loss: 0.0877 - acc: 0.9792 - val_loss: 0.1248 - val_acc: 0.9583\n",
      "Epoch 272/300\n",
      "96/96 [==============================] - 0s 44us/step - loss: 0.0873 - acc: 0.9792 - val_loss: 0.1244 - val_acc: 0.9583\n",
      "Epoch 273/300\n",
      "96/96 [==============================] - 0s 46us/step - loss: 0.0868 - acc: 0.9792 - val_loss: 0.1239 - val_acc: 0.9583\n",
      "Epoch 274/300\n",
      "96/96 [==============================] - 0s 43us/step - loss: 0.0864 - acc: 0.9792 - val_loss: 0.1234 - val_acc: 0.9583\n",
      "Epoch 275/300\n",
      "96/96 [==============================] - 0s 29us/step - loss: 0.0859 - acc: 0.9792 - val_loss: 0.1230 - val_acc: 0.9583\n",
      "Epoch 276/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.0855 - acc: 0.9792 - val_loss: 0.1226 - val_acc: 0.9583\n",
      "Epoch 277/300\n",
      "96/96 [==============================] - 0s 61us/step - loss: 0.0851 - acc: 0.9792 - val_loss: 0.1222 - val_acc: 0.9583\n",
      "Epoch 278/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.0847 - acc: 0.9792 - val_loss: 0.1219 - val_acc: 0.9583\n",
      "Epoch 279/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.0843 - acc: 0.9792 - val_loss: 0.1216 - val_acc: 0.9583\n",
      "Epoch 280/300\n",
      "96/96 [==============================] - 0s 34us/step - loss: 0.0839 - acc: 0.9792 - val_loss: 0.1212 - val_acc: 0.9583\n",
      "Epoch 281/300\n",
      "96/96 [==============================] - 0s 50us/step - loss: 0.0835 - acc: 0.9792 - val_loss: 0.1209 - val_acc: 0.9583\n",
      "Epoch 282/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.0831 - acc: 0.9792 - val_loss: 0.1205 - val_acc: 0.9583\n",
      "Epoch 283/300\n",
      "96/96 [==============================] - 0s 25us/step - loss: 0.0827 - acc: 0.9792 - val_loss: 0.1201 - val_acc: 0.9583\n",
      "Epoch 284/300\n",
      "96/96 [==============================] - 0s 53us/step - loss: 0.0823 - acc: 0.9792 - val_loss: 0.1197 - val_acc: 0.9583\n",
      "Epoch 285/300\n",
      "96/96 [==============================] - 0s 42us/step - loss: 0.0819 - acc: 0.9792 - val_loss: 0.1193 - val_acc: 0.9583\n",
      "Epoch 286/300\n",
      "96/96 [==============================] - 0s 23us/step - loss: 0.0816 - acc: 0.9792 - val_loss: 0.1189 - val_acc: 0.9583\n",
      "Epoch 287/300\n",
      "96/96 [==============================] - 0s 30us/step - loss: 0.0812 - acc: 0.9792 - val_loss: 0.1186 - val_acc: 0.9583\n",
      "Epoch 288/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.0808 - acc: 0.9792 - val_loss: 0.1183 - val_acc: 0.9583\n",
      "Epoch 289/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.0805 - acc: 0.9792 - val_loss: 0.1180 - val_acc: 0.9583\n",
      "Epoch 290/300\n",
      "96/96 [==============================] - 0s 32us/step - loss: 0.0801 - acc: 0.9792 - val_loss: 0.1176 - val_acc: 0.9583\n",
      "Epoch 291/300\n",
      "96/96 [==============================] - 0s 54us/step - loss: 0.0798 - acc: 0.9792 - val_loss: 0.1173 - val_acc: 0.9583\n",
      "Epoch 292/300\n",
      "96/96 [==============================] - 0s 38us/step - loss: 0.0795 - acc: 0.9792 - val_loss: 0.1170 - val_acc: 0.9583\n",
      "Epoch 293/300\n",
      "96/96 [==============================] - 0s 24us/step - loss: 0.0791 - acc: 0.9792 - val_loss: 0.1166 - val_acc: 0.9583\n",
      "Epoch 294/300\n",
      "96/96 [==============================] - 0s 19us/step - loss: 0.0788 - acc: 0.9792 - val_loss: 0.1163 - val_acc: 0.9583\n",
      "Epoch 295/300\n",
      "96/96 [==============================] - 0s 35us/step - loss: 0.0785 - acc: 0.9792 - val_loss: 0.1160 - val_acc: 0.9583\n",
      "Epoch 296/300\n",
      "96/96 [==============================] - 0s 28us/step - loss: 0.0782 - acc: 0.9792 - val_loss: 0.1157 - val_acc: 0.9583\n",
      "Epoch 297/300\n",
      "96/96 [==============================] - 0s 31us/step - loss: 0.0779 - acc: 0.9792 - val_loss: 0.1154 - val_acc: 0.9583\n",
      "Epoch 298/300\n",
      "96/96 [==============================] - 0s 36us/step - loss: 0.0775 - acc: 0.9792 - val_loss: 0.1151 - val_acc: 0.9583\n",
      "Epoch 299/300\n",
      "96/96 [==============================] - 0s 33us/step - loss: 0.0772 - acc: 0.9792 - val_loss: 0.1148 - val_acc: 0.9583\n",
      "Epoch 300/300\n",
      "96/96 [==============================] - 0s 26us/step - loss: 0.0769 - acc: 0.9792 - val_loss: 0.1146 - val_acc: 0.9583\n"
     ]
    }
   ],
   "source": [
    "history_dict = {}\n",
    "for model in models:\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer = Adam(),\n",
    "        metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        train_x, train_y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=300, verbose=True,\n",
    "        validation_data=(validate_x, validate_y))\n",
    "    history_dict[model.name] = history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAF3CAYAAAC8MNLCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xl8VPW5+PHPkz0hKwlhS0JQQBaJrApuVVGL2opLF6mt+7Wttbe3tvfW/tqKdqHLrfeqvVpLFbWKS11LK1rrVuqKbLKjEUJISAhk39fn98c5CQmS5CSZySQzz/v1mtfMOfM9Z545jjz5fs93EVXFGGOMMcNfWKADMMYYY4xvWFI3xhhjgoQldWOMMSZIWFI3xhhjgoQldWOMMSZIWFI3xhhjgoTfkrqIrBSREhHZ1s37IiL3iEiuiGwRkTn+isUYY4wJBf6sqT8MLO7h/QuAye7jRuD3fozFGGOMCXp+S+qquhYo66HIEuBP6ngPSBaRsf6KxxhjjAl2gbynPh7Y32m7wN1njDHGmH6ICHQAXojIjThN9IwYMWLu1KlTAxyRMcYYM3g2bNhwWFVH9VYukEm9EMjstJ3h7vsUVV0BrACYN2+erl+/3v/RGWOMMUOEiOzzUi6Qze+rgavcXvALgEpVLQpgPMYYY8yw5reauog8AZwFpIlIAbAMiARQ1fuBNcCFQC5QB1zrr1iMMcaYUOC3pK6qS3t5X4Fv+evzjTHGmFAzLDrKGT84uB2KPgx0FMPLhNMgZUKgozABsrWgkt0HqwMdht9FNVUw5uBahLZAhzKsnXTBDURFxwz651pSD1XPfx2KtwY6iuHl+EXwtecCHYUJAFXl6ofWUVbbFOhQ/O4nEY9yccRLgQ5j2Ks6a6kldTOIGqth6ufgs78IdCTDwxu/hN1roK0NwmzJhFCz53AtZbVN/PCCqVw4M7jnyEp/6jc0hs2n9Pz/C3Qow9qYhOSAfK4l9VDV0ghxIyElO9CRDA/HfQa2PAmHd0P6tEBHYwbZpvwKAM6emk7myLgAR+NHzfVwaBuc+m3GTbT5QIYjq3KEqpYGiBj8pqFhK+Nk53n/usDGYQJiY345CdERTBoVH+hQ/KvoQ2hrOfJ7N8OOJfVQ1dIIEdGBjmL4SD0eYlOgwJJ6KNq4r5xZWcmEhUmgQ/Gv9j9aM+YHNg7Tb9b8HopUrabeVyLOP3T73oG8twIdjfGR6oZm8svqeyzT2qYkl+zk8xnjIa95kCILkNxXnVty8b3ORmqGKEvqoaitBbTNaup9NeFU+PgVePiiQEdifCQBmOGh3JNRwDb3EexmXRnoCMwAWFIPRc1uzcRq6n2z4CbnXqO2BjoS4yP/8dRmRsVHsWR2zwtERkeEMWlUPCJB3vwOMHZWoCMwA2BJPRS1NDrPltT7JiIask8LdBTGR1SVl2pquTonmxNPsxENJjhYR7lQ1NLgPFvzuwlh5XXNNLa0MSbR/rg1wcOSeiiymroxFFU6t6HGJtn/ByZ4WFIPRR01dfvHzISu4krn/4OxybEBjsQY37GkHoqspm4MB9qTutXUTRCxpB6K7J66MRRX1hMRJqTF2/8HJnhYUg9F1vxuDEWVDYxOjCE82GeJMyHFknoo6mh+txqKCV1FFQ2MsaZ3E2QsqYeiFpt8xpjiqga7n26CjiX1UGQ1dRPiVJWiynpL6ibo2IxyocjuqZsQU1LVwPl3raWy3lmQRdXZP86Gs5kg02tSF5FwVZvsOqhYTd2EmHf3lFJR18zVCyeQFBsJQER4GBefNC7AkRnjW15q6h+LyLPAQ6q6oy8nF5HFwN1AOPCAqv7qqPezgEeAZLfMraq6pi+fYfqhvaYeabUUExo27isnLiqcn3xuOhHhdtfRBC8vv+6TgI+AB0TkPRG5UUQSeztIRMKBe4ELgOnAUhGZflSxHwN/VtXZwBXAfX2K3vRPe0093GrqJjRszK/gpIxkS+gm6PX6C1fValX9o6qeCvwAWAYUicgjIjKph0NPBnJVdY+qNgFPAkuOPj3Q/gdCEnCgz9/A9F1LA4RHQZj9A2eCX31TKzuLqpgzITnQoRjjd57uqQMXAdcC2cCdwCrgDGANMKWbQ8cD+zttFwCnHFXmduAVEfk2MAI413vopt9aGq2TnAkZWwsraWlTZmemBDoUY/zO0z114A3gv1X1nU77nxGRMwf4+UuBh1X1ThFZCDwqIieqalvnQiJyI3AjQFZW1gA/sh/eWA4fPtl7uah4uPLPkJTh/5gGoqXBOsmZkPBfz3zIqztLAJidZTV1E/y8JPUcVa051huq+u89HFcIZHbaznD3dXY9sNg917siEgOkASVHfc4KYAXAvHnz1EPMvrXjL4BC1qndl2lthO3Pw8f/gHnXDlpo/dLcYDV1E/RUlb9+WETWyDiuP30iqTbHuwkBXpL6vSLyHVWtABCRFOBOVb2ul+M+ACaLyEScZH4F8JWjyuQDi4CHRWQaEAMc6ssXGBSVhTD7Srjg192XUYW9a6Hgg6Gf1K2mbkJAZX0z9c2tfHFeBjeccVygwzFmUHjpKZXTntABVLUcmN3bQaraAtwM/B3YidPLfbuI/FRELnaLfQ/4NxH5EHgCuEZVB78m3pOGKmiqhsRexrOKQMZ8J6kPdXZP3YSAoo6lVW3opgkdXmrqYSKS4iZzRGSkx+Nwx5yvOWrfbZ1e7wBO8x5uAFS5HfITx/deNmM+fPQy1JVB3Ej/xjUQVlM3IaC4Pakn2x+wJnR4Sc53Au+KyNOAAF8AfuHXqIaSqgLn2WtSByjcAJPP819MA9XSCBFWezHB7UCls3CRze9uQomXcep/Ai4HDgLFwGWq+qi/Axsy2mvqSR6S+vi5IGHw0g/g7z/yb1wDYTV1EwKKKxsIExhlHeRMCPE0+4iqbgf+DKwGatzpXUND1QFAIH5M72Wj42HhzdDaBO/dB21tvR8TCHZP3YSAosoG0hNibBY5E1J6/bWLyMUi8jGwF/gnkAe85Oe4ho7KAohPh4gob+XP/xksuAm0DRor/Rtbf1lN3YSAosp6u59uQo6XP2F/BiwAPlLViThD0N7za1RDSdUBb/fTO2vvJFdX5vt4fMFq6iYEFFU22P10E3K8JPVmVS3F6QUfpqpvAPP8HNfQUXWg9+FsR4t1k3p9ue/j8YWWequpm6CmqhRXNjAm0TqEmtDipfd7hYjEA2uBVSJSAtT6N6whpKoQJvZxNlyrqRsTUFX1LdQ1tTLOmt9NiPGS1JcA9cB3gStxVlP7qT+DGjIaqqCxylvP985i3YUj6odqUrd76iZ4bdhXzsq39gIwxprfTYjpMam7K7T9TVXPBtqARwYlqqHi0G7nOWVi345rT+pDsabe1ub0zo+0ZkkTnO57I5d/fXyY40aN4KQMW8TFhJYek7qqtopIm4gkqeoQ7crtR+1TvrZPKuNVTLIzXn0o1tRbG51nq6mbIKSqbNpfwZJZ4/jvL54U6HCMGXRemt9rgK0i8g863UvvZYW24FCwDpIyIXFs344LC3MS+1Csqbc4U2faPXUTjPaV1lFW28ScCbZ2uglNXpL6c+4j9Oz/ADJP7t+xcSOHZk29xWrqJnhtzHdGnNja6SZU9ZrUVTW07qO3qzrgzPueeXP/jo8daTV1YwbZpvwK4qMjmJyeEOhQjAmIXpO6iOwFPrUcqqoG9wLF+9c5zxkDqKlXFR7zrfLaJp78YD/NrV2nkZ106FVS6vMAiAwXZmWmEBEmzpsR0TDvemcq2s7qymDDw9DW4i2uutIj5zPDQm5JDWu2FgU6jGHhjd0lzMpMJrz9/xtjQoyX5vfOE83EAF8EhvC6oj5yaJfzPObE/h0fOxKKtx3zrb9tOcCvX97VZV88dXwY/UPCpdPfT3uOcc45X+u6b8PD8NodfYstPBpGBvffZMHkt3/fzcvbiwMdxrBxw+l9HK1iTBDx0vxeetSuu0RkA3DbscoHjboyiE7qf402NqXbe+qltU0A7P75YiLC3En99rxJ+GNK61eeoSnrM8z62Stce1o2ty6eBqrw20lOx72jk3rBB5A6Gb71fh+CE6cznxnyVJWN+eUsmTWO//nSrECHMyxYLd2EMi/N73M6bYbh1Ny91PCHt/oyiBtAD9q4FGiug+YGiOx6/7qirpmEmAiiI8KP7Cz8ABDCs04mNiaKaeNS2JhfDWFumYz5Tse9zlSd2wRTPnuknAkqhRX1lFQ3MndCiiUrY0yvvCTnOzu9bsFZre1L/glnCKkrOzKHe390zP9eBpFd544vr2siJe6oVd/2r4NRJ0BMEuD03n1iXT7NrW1Ehoc5Sf3jf0BDZUcZyvdC3WHICJ2p+EPNxvwKAGZn2hAtY0zvem2DVdWzOz3OU9UbVXX3YAQXUPVlR+Zw74+47hd1Ka9rJiUu8sgOVacZvdMkN3OyUmhobmNXUbWzI2M+oFC44chxBevd9/rZmc8MeZvyy4mJDGPqWOvNbYzpnZfm9+XAb1S1wt1OAb6nqj/2d3ABVVfm3Kvur9juF3WpOLqmXpoLDRVdxsS3T57x2Hv7+NL8TOaOnwsIbHoMakqcQtuehah4SJ/W/zhDWEF5Hev2DsFhh5386+PD5GQkO601xhjTCy/N7xeo6v9r31DVchG5EOg1qYvIYuBuIBx4QFV/dYwyXwJuxxk296GqfsVj7P5VXz6wmvqINOe59tCn3iqva+K4tBFHdnQMnztSUx+XFEN2ahxPrd/PC5sL+XDZ+cSMn+sk8m3PHjl2ymK7n95PP3p+G//86NP/fYaa/zh3AH9cGmNCipekHi4i0araCCAisUCvXcLdxWDuBc4DCoAPRGS1qu7oVGYy8EPgNPePhfT+fAmfa212VmcbyD31BHdq2epPjy+uqG0muXNNvWCd09M+7YSOXSLC3/79DFZvPsD/e34r2w9UMffqv0LNUUObEvu4gpwBoK3N6VV+yaxxfPe8KYEOp1thIoxPtsV3jDHeeEnqq4DXROQhd/tavK3WdjKQq6p7AETkSZxlXHd0KvNvwL2qWg6gqiVeA/er9vvgA6mpx6ZARKwzM10nza1tVDe2dG1+L1gPGXM/NcwsPjqCc6el8/+ed+6tzp1wnI0v95HcQzVUN7Rw2qQ0JqSO6P0AY4wZBryMU/+1iHwInOvu+pmq/t3DuccD+zttFwCnHFVmCoCIvI3TRH+7qr7s4dz+1Z7UYwfQ41jEWYe9sqDL7oq6ZgBSRrgd5RqroWQHTL3omKdJT4xhfHIsm9xe0MY3NrlzhNvCH8aYYOKlo9xE4M32ZCsisSKSrap5Pvr8ycBZQAawVkRmtnfK6xTDjcCNAFlZWT742F60d24bSFIHSBz3qZp6Zb0z8UxH83vhBtC2Hnuwz5mQwvq8od2ha7jZuK+C5LjIrn0bjDFmmPPS/P40cGqn7VZ3X2+LjBcCmZ22M9x9nRUA76tqM7BXRD7CSfJdZllR1RXACoB58+Z9ah56n2ufCW4gze/g3O/eu7bLrvL2mnr7kLaONdvndnuaOVnJ/PXDA7y64yBJnYfCdSMhJoKpYxI/tb+huZVthZWfnsg/BK3LK2N2ZjIiNqGLMSZ4eEnqEara1L6hqk0iEtXTAa4PgMluTb8QuAI4umf7C8BS4CERScNpjj96xvPB11FT90FSry6G1hYIdy51uTtFbHKsewn3f+B0kOuhVWB+thPHDX9a7/mjX/numUwZ3XVs8/+++hF/+GfgL+9Q8cV5GYEOwRhjfMpLUj8kIher6moAEVkCHO7tIFVtEZGbgb/j3C9fqarbReSnwHr3fH8HzheRHTgtAP95jLnmB5/PaurjQFuhtsR5zZF76slxkUcmnTnhwh5Pc+L4JP7yrdOobuh9Jbbyuia+/cQm3t9b9qmk/v6eMmaMS+SHF9i49rAwZ4IfY4wJJl6S+jeAVSLyf4DgdH67ysvJVXUNsOaofbd1eq3ALe5j6Kgrg7BIZ2KXgUhya4KVhR1JvbzOqamnjIiC0k+cPyAye7uTASdlJnv6SFXljr9uZ9O+cr62YELH/obmVrYfqOS60yZy+uS0Pn4RY4wxw4GX3u+fAAtEJN7drhGR0X6PLJDap4gd6P1WN5E766o7ibu8rpnIcGFEVHin++m+m+ZVRJidlcKm/V17y28/UEVzqzLbaqfGGBO0+jL3ZATwZRF5Ddjkp3iGhoEu5tKufWKYTj3gK+qaSI6LcjpoFayDqARnIRcfmpOVwt7DtZTVdnSFODKEK8tbjd8YY8zw02NN3Z09bglOB7fZQAJwCbC2p+OGvfqKgQ9ngyMT0Bz+yGmCB7SqkMkxdc52/vvupDO+neZ1tpu439xdwsLjUwF4b08Z45NjSU+M6elQY4wxw1i3SV1EHgfOAF4Bfge8jjND3JuDE1oA1Zf5ZuY2EUjOgg0POQ/g1+3v/a/7PO3WgX/OUXIykogMF27584dd9i+ZNa6bI4wxxgSDnmrq04FyYCewU1VbRSQ0hjjXlcH47seN98nlf4QDmzs2f/P33WSmxLL05Cynhj71c775nE7ioiL403WnsK+0tmOfCJw5ZZTPP8sYY8zQ0W1SV9VZIjIVZxz5qyJyGEgQkdGqenDQIhxsqgNfS72zsSc5D5ye6Q88/zLXzsqGuf4dVrbw+NSOpndjjDGhoceOcqq6S1WXqepU4Ds4C7l8ICLvDEp0gdBUC61Nvukod5Sy2iaaWtoYk2T3tY0xxviel3HqAKjqBmCDiPwnzr324OSriWeOoaiyAYCxSbaUpjHGGN/znNTbuRPGBG/vd19NEXsMxR1J3WrqxhhjfK/PST3o+bim/vVH1/PqzhIyU2K5amE2YEndGGOMf/Rl8pnQ4MOaekNzK6/tLGFMYgx5pXW8vquEiDAhNT56wOc2xhhjjuZlPfVo4HIgu3N5Vf2p/8IKoHpn5jVfTD6ztbCSljblG2cdz09e2Ma7e0oZkxhDeJgt92mMMcb3vNTU/4Izq1wLUNvpEZx8mNQ37nPOdcGJYxibFENrm1rTuzHGGL/xck89Q1UX+z2SoaKuzJmPPcLLkvE925hfTtbIONLio5mdlUzR1mIbzmaMMcZvvNTU3xGRmX6PZKioL4O4gdfSVZWN+RUdC6i0r909LtmGsxljjPEPLzX104FrRGQv0Iizprqqao5fIwsUH63QVlhRz6HqRuZMcJJ5+5Kno21BFWOMMX7iJalf4PcohhIfTRG7Md9Zz7y9hj4rM5nvLJrMRTPHDvjcxhhjzLH0mtRVdZ+InMSRWeT+paof9nTMsFZXBikTB3yaTfnlxESGccKYBADCw4TvnjdlwOc1xhhjutPrPXUR+Q6wCkh3H4+JyLf9HVjA+LCmnpORTGS4TQVgjDFmcHhpfr8eOEVVawFE5NfAuzhrrAeXtlZoqBzwcLaG5lZ2HKjk+tN9sCa7McYY45GXaqQArZ22W919wafeuQ8+0I5y2woraW5VZrs9340xxpjB4CWpPwS8LyK3i8jtwHvAg15OLiKLRWS3iOSKyK09lLtcRFRE5nmK2l98NO/7pqM6yRljjDGDwUtHuf8RkTdxhrYBXKuqm3o7TkTCgXuB84ACnHXYV6vqjqPKJeCs1f5+H2P3vUO7neeU7AGdZmN+OZkjYxmVYHO8G2OMGTzd1tRFJNF9HgnkAY+5j33uvt6cDOSq6h5VbQKexJlu9mg/A34NNPQtdD8oWAfhUTD2pH6fwpl0ptxq6cYYYwZdT83vj7vPG4D1nR7t270ZD+zvtF3g7usgInOATFV9sacTiciNIrJeRNYfOnTIw0f30/4PYEwORPS/hl1U2cDBqkZmZ9r9dGOMMYOr2+Z3Vf2c+zzwQdvHICJhwP8A1/RWVlVXACsA5s2bp/6Ih9ZmOLAJ5l07oNNszHcWcWmfSc4YY4wZLF7Gqb/mZd8xFAKZnbYz3H3tEoATgTdFJA9YAKwOWGe54q3QUg8Z8wd0mo37KoiOCGPa2EQfBWaMMcZ4021NXURigDggTURSODKMLZGjmtG78QEwWUQm4iTzK4CvtL+pqpVAWqfPexP4vqp6adr3vQL3YzNPHtBpNu0vJycjySadMcYYM+h66v3+deA/gHE499Hbk3oV8H+9nVhVW0TkZuDvQDiwUlW3i8hPgfWqunpAkfvaoZ3OpDOJXv5e6d7HB2v4wtwMHwVljDHGeNfTPfW7gbtF5Nuq2q/Z41R1DbDmqH23dVP2rP58hs9UFkJSBkj/59WpbmimprGFccm2EpsxxpjB52Wc+u9E5ERgOhDTaf+f/BnYoKs64CT1ASiqdEbljU2yNdONMcYMvl6TuogsA87CSeprcJZifQsIsqReOOD76UeSutXUjTHGDD4vvbm+ACwCilX1WuAkIMmvUQ22pjpnitjEcQM6TVFFPQBjLKkbY4wJAC9JvV5V24AWd5a5EroOVRv+qoucZx80v4vA6ERL6sYYYwafl6VX14tIMvBHnF7wNThLrwaPygLneYA19eLKBkbFR9twNmOMMQHhpaPcTe7L+0XkZSBRVbf4N6xBVnXAeR7gcLYDlfV2P90YY0zA9DT5zJye3lPVjf4JKQCq3InufFBTP35UvA8CMsYYY/qup5r6ne5zDDAP+BBnApocnAVdFvo3tEFUVQixIyFyYEPRiisbOG1SWu8FjTHGGD/o9uavqp6tqmcDRcAcVZ2nqnOB2XSdw334qzow4Kb36oZmqm3iGWOMMQHkpaPcCaq6tX1DVbeJyDQ/xjR4qg/C3rVwaBeMmjqgUxW7Y9TH2MQzxhhjAsRLUt8iIg8Aj7nbVwLB0VHu0C547gbn9fQlAzqVTTxjjDEm0Lwk9WuBbwLfcbfXAr/3W0SDKWM+3LzBme89JXtApyqqdCaesaRujDEmULwMaWsA/td9BJeoOEib5JNTtU88k55gSd0YY0xg9DSk7c+q+iUR2Qro0e+rao5fIxtmiisbSIuPJirCJp4xxhgTGD3V1Nub2z83GIEMdwcqGxhnTe/GGANAc3MzBQUFNDQ0BDqUYSUmJoaMjAwiIyP7dXxP66kXuc/7+hlbSCmurGdi2ohAh2GMMUNCQUEBCQkJZGdnIyKBDmdYUFVKS0spKChg4sSJ/TpHt23FIlItIlXHeFSLSFW/ow5SRZUNto66Mca4GhoaSE1NtYTeByJCamrqgFo3eqqpJ/T7rCGmprGF6oYW6/lujDGdWELvu4FeMy9D2to/KB1nylgAVDV/QJ8cRIorbR11Y4wxgddrV20RuVhEPgb2Av8E8oCX/BzXsHKgwmkqGZdsze/GGBOMsrOzOXz48IDKXHfddaSnp3PiiSf6OrwOXsZf/QxYAHykqhOBRcB7Xk4uIotFZLeI5IrIrcd4/xYR2SEiW0TkNRGZ0Kfoh4iOKWITraZujDHm2K655hpefvllv36Gl6TerKqlQJiIhKnqGzirtvVIRMKBe4ELgOnAUhGZflSxTcA8d8z7M8Bv+hR9gFXUNVFS1cAnh2oQgdGW1I0xZsjIy8tj6tSpXHPNNUyZMoUrr7ySV199ldNOO43Jkyezbt06ysrKuOSSS8jJyWHBggVs2eLMgl5aWsr555/PjBkzuOGGG1A9Ml3LY489xsknn8ysWbP4+te/Tmtrq6d4zjzzTEaOHOmX79rOyz31ChGJx5kedpWIlAC1Ho47GchV1T0AIvIksATY0V7A/QOh3XvAV70GHmjvflLK0j8eabAYkxhjE88YY8wx3PHX7ew44NtBU9PHJbLs8zN6LZebm8vTTz/NypUrmT9/Po8//jhvvfUWq1evZvny5WRmZjJ79mxeeOEFXn/9da666io2b97MHXfcwemnn85tt93Giy++yIMPPgjAzp07eeqpp3j77beJjIzkpptuYtWqVVx11VU+/X795SWpLwEagO/iLOaSBPzUw3Hjgf2dtguAU3oofz3D6F79rmLnB/rji6YRGxXO1DGJAY7IGGPM0SZOnMjMmTMBmDFjBosWLUJEmDlzJnl5eezbt49nn30WgHPOOYfS0lKqqqpYu3Ytzz33HAAXXXQRKSkpALz22mts2LCB+fPnA1BfX096enoAvtmx9TRN7L3A46r6dqfdj/gjCBH5Kk6T/me6ef9G4EaArKwsf4TQZ0WVDURFhHH96RNt2IYxxvTAS43aX6Kjozteh4WFdWyHhYXR0tLS55nbVJWrr76aX/7ylz6N01d6ai/+CPitiOSJyG9EZHYfz10IZHbaznD3dSEi5wI/Ai5W1cZjnUhVV6jqPFWdN2rUqD6G4R/OZDMxltCNMWYYO+OMM1i1ahUAb775JmlpaSQmJnLmmWfy+OOPA/DSSy9RXl4OwKJFi3jmmWcoKSkBoKysjH37hs7Eq90mdVW9W1UX4tSeS4GVIrJLRJaJyBQP5/4AmCwiE0UkCrgCWN25gPuHwh9wEnpJv79FABRX1ltvd2OMGeZuv/12NmzYQE5ODrfeeiuPPOI0SC9btoy1a9cyY8YMnnvuuY5W4unTp/Pzn/+c888/n5ycHM477zyKioo8fdbSpUtZuHAhu3fvJiMjo+M+vS9J5x59vRZ2kvBKIEdVwz2UvxC4CwgHVqrqL0Tkp8B6VV0tIq8CM4H2K5Kvqhf3dM558+bp+vXrPcfsL6f96nVOnjiS//3yrECHYowxQ87OnTuZNm1aoMMYlo517URkg6r2OvKs145yIhKBMyztCpwx6m8Ct3sJTFXXAGuO2ndbp9fnejnPUNPWphysarAZ5IwxxgwpPXWUOw9YClwIrAOeBG5UVS/D2YLa4ZpGWtrUllo1xhjTRWlpKYsWLfrU/tdee43U1FS/f35PNfUfAo8D31PVcr9HMowUtc8gZ6uyGWOM6SQ1NZXNmzcH7PN7WqXtnMEMZDhpT+q2KpsxxpihxKZA64cid1U2S+rGGGOGEkvq/VDsTjwzckRUoEMxxhhjOnheT91AS2sb1z78AZvzKxiTaBPPGGOMGVqspt4HO4uq+dfHh5kxPpFvnzMp0OEYY4wZJANdT33//v2cffbZTJ8+nRkzZnD33Xf7I0yrqffFpv3OIIDffvEkMlLiAhyNMcaY4SIiIoI777yTOXPmUF1dzdy5cznvvPOYPv3oFckH+Dk+PVuQ27i0juwtAAAgAElEQVSvnPSEaMYn21A2Y4zx7KVboXirb885ZiZc8Ksei+Tl5bF48WIWLFjAO++8w/z587n22mtZtmwZJSUlrFq1ikmTJnHdddexZ88e4uLiWLFiBTk5OZSWlrJ06VIKCwtZuHDhp9ZTv+eee2hqauKUU07hvvvuIzy850lWx44dy9ixYwFISEhg2rRpFBYW+jypW/N7H2zMr2BOVordSzfGmGEiNzeX733ve+zatYtdu3Z1rKf+29/+luXLl7Ns2TJmz57Nli1bWL58ece66O3rqW/fvp1LL72U/Px8oOt66ps3byY8PLxjQRiv8vLy2LRpE6ec0tNq5P1jNXWPDtc0kl9Wx5WnDI2lX40xZtjopUbtT0NtPfWamhouv/xy7rrrLhITE335VYEQT+of7q/gVy/t4leXz6Swop4//HMP3S1vU1nfDMCcCSmDF6AxxpgBGUrrqTc3N3P55Zdz5ZVXctlll/X5eC9Cuvk9OjKMd/eUsmFfOY++u48P8sqoqm8+5kOA86aPJicjKdBhG2OM8ZHBWk9dVbn++uuZNm0at9xyi5++TYjX1CenJxAfHcHG/HI25Vdw7rTR3LN0dqDDMsYYM0huv/12rrvuOnJycoiLi+uynvrSpUuZMWMGp5566jHXU29rayMyMpJ7772XCRMm9Pg5b7/9No8++igzZ85k1ixnye7ly5dz4YUX+vT79Gk99aHA1+upf/WB9/noYDUl1Y3c/vnpXHPaRJ+d2xhjQpWtp95/A1lPPaSb3wFmZyVTUt3ovrb75cYYY4avkG5+B5jjJvLoiDCmjfV9T0RjjDGhYyivpx4SZmclA5CTkURURMg3XBhjjBmAIbueeqhIjotiyaxxLDzO/39BGWNMKFFVm6yrjwbazy3kkzrA3VdYj3djjPGlmJgYSktLSU1NtcTukapSWlpKTExMv8/h16QuIouBu4Fw4AFV/dVR70cDfwLmAqXAl1U1z58xGWOM8b+MjAwKCgo4dOhQoEMZVmJiYsjIyOj38X5L6iISDtwLnAcUAB+IyGpV3dGp2PVAuapOEpErgF8DX/ZXTMYYYwZHZGQkEyfaEOHB5s+eYScDuaq6R1WbgCeBJUeVWQI84r5+Blgk1k5jjDHG9Is/k/p4YH+n7QJ33zHLqGoLUAlYjzVjjDGmH4bFGC4RuVFE1ovIers/Y4wxxhybPzvKFQKZnbYz3H3HKlMgIhFAEk6HuS5UdQWwAkBEDolI77Pne5cGHPbh+YY7ux5d2fXoyq5HV3Y9urLr0ZUvr0fPk8u7/JnUPwAmi8hEnOR9BfCVo8qsBq4G3gW+ALyuvQzSU9VRvgxSRNZ7mU83VNj16MquR1d2Pbqy69GVXY+uAnE9/JbUVbVFRG4G/o4zpG2lqm4XkZ8C61V1NfAg8KiI5AJlOInfGGOMMf3g13HqqroGWHPUvts6vW4AvujPGIwxxphQMSw6yvnZikAHMMTY9ejKrkdXdj26suvRlV2Prgb9egy79dSNMcYYc2xWUzfGGGOCREgndRFZLCK7RSRXRG4NdDyBICJ5IrJVRDaLyHp330gR+YeIfOw+pwQ6Tn8RkZUiUiIi2zrtO+b3F8c97u9li4jMCVzk/tHN9bhdRArd38hmEbmw03s/dK/HbhH5bGCi9g8RyRSRN0Rkh4hsF5HvuPtD8vfRw/UI1d9HjIisE5EP3etxh7t/ooi8737vp0Qkyt0f7W7nuu9n+yUwVQ3JB06P/E+A44Ao4ENgeqDjCsB1yAPSjtr3G+BW9/WtwK8DHacfv/+ZwBxgW2/fH7gQeAkQYAHwfqDjH6TrcTvw/WOUne7+fxMNTHT/fwoP9Hfw4bUYC8xxXycAH7nfOSR/Hz1cj1D9fQgQ776OBN53/7v/GbjC3X8/8E339U3A/e7rK4Cn/BFXKNfUvcxNH6o6z8n/CHBJAGPxK1VdizOcsrPuvv8S4E/qeA9IFpGxgxPp4OjmenRnCfCkqjaq6l4gF+f/q6CgqkWqutF9XQ3sxJnaOiR/Hz1cj+4E++9DVbXG3Yx0Hwqcg7OWCXz69+H3tU5COal7mZs+FCjwiohsEJEb3X2jVbXIfV0MjA5MaAHT3fcP5d/MzW6T8spOt2NC5nq4TaWzcWpjIf/7OOp6QIj+PkQkXEQ2AyXAP3BaIyrUWcsEun7nQVnrJJSTunGcrqpzgAuAb4nImZ3fVKetKGSHSIT693f9HjgemAUUAXcGNpzBJSLxwLPAf6hqVef3QvH3cYzrEbK/D1VtVdVZONOgnwxMDXBIIZ3UvcxNH/RUtdB9LgGex/lhHmxvNnSfSwIXYUB09/1D8jejqgfdf7zagD9ypAk16K+HiETiJLBVqvqcuztkfx/Huh6h/Ptop6oVwBvAQpzbLu0Tu3X+zh3XQ3pY62SgQjmpd8xN7/ZOvAJnLvqQISIjRCSh/TVwPrCNI3Py4z7/JTARBkx33381cJXby3kBUNmpGTZoHXVf+FKc3wg41+MKt1fvRGAysG6w4/MX937ng8BOVf2fTm+F5O+ju+sRwr+PUSKS7L6OBc7D6WfwBs5aJvDp30f778bTWif9EugehIF84PRW/QjnPsiPAh1PAL7/cTi9Uz8EtrdfA5z7PK8BHwOvAiMDHasfr8ETOE2GzTj3v67v7vvj9Ha91/29bAXmBTr+Qboej7rfdwvOP0xjO5X/kXs9dgMXBDp+H1+L03Ga1rcAm93HhaH6++jheoTq7yMH2OR+723Abe7+43D+eMkFngai3f0x7nau+/5x/ojLZpQzxhhjgkQoN78bY4wxQcWSujHGGBMkLKkbY4wxQcKSujHGGBMkLKkbY4wxQcKSujHGGBMkLKkbY4wxQcKSujHGGBMkLKkbY4wxQcKSujHGGBMkLKkbY4wxQcJvSV1EVopIiYhs66XcfBFpEZEv9FTOGGOMMT3zZ039YWBxTwVEJBz4NfCKH+MwxhhjQoLfkrqqrgXKein2beBZoMRfcRhjjDGhIiJQHywi44FLgbOB+V6PS0tL0+zsbH+FZYwxxgw5GzZsOKyqo3orF7CkDtwF/EBV20Skx4IiciNwI0BWVhbr168fhPCMMcaYoUFE9nkpF8ikPg940k3oacCFItKiqi8cXVBVVwArAObNm6eDGqUxxhgzTAQsqavqxPbXIvIw8LdjJXRjjDHGeOO3pC4iTwBnAWkiUgAsAyIBVPV+f32uMcYYE6r8ltRVdWkfyl7jrzh6UlRZz2Pv7eNbZ08iLiqQdyKMMSa4NDc3U1BQQENDQ6BDGVZiYmLIyMggMjKyX8eHdCYrKK/n3jc+ISMljqUnZwU6HGOMCRoFBQUkJCSQnZ1Nb52hjUNVKS0tpaCggIkTJ/Z+wDGE9DSx8yakMHVMAo++uw9V639njDG+0tDQQGpqqiX0PhARUlNTB9S6EdJJXUS4amE2O4qq2JhfHuhwjDEmqFhC77uBXrOQTuoAS2aNIyE6gj+962kIoDHGGDNkhXxSHxEdweVzM1iztYhD1Y2BDscYY8wQlJ2dzeHDhwdU5rrrriM9PZ0TTzzR1+F1CPmkDvDVBRNoblWe+iA/0KEYY4wJUtdccw0vv/yyXz8jpHu/t5uUHs9pk1JZ9X4+X//M8USG2986xhjjK3f8dTs7DlT59JzTxyWy7PMzeiyTl5fH4sWLWbBgAe+88w7z58/n2muvZdmyZZSUlLBq1SomTZrEddddx549e4iLi2PFihXk5ORQWlrK0qVLKSwsZOHChV06Uz/22GPcc889NDU1ccopp3DfffcRHh7ea8xnnnkmeXl5A/3qPbLs5brm1IkUVTawZmtRoEMxxhjjI7m5uXzve99j165d7Nq1i8cff5y33nqL3/72tyxfvpxly5Yxe/ZstmzZwvLly7nqqqsAuOOOOzj99NPZvn07l156Kfn5Tkvuzp07eeqpp3j77bfZvHkz4eHhrFq1KpBfsQurqbsWTU3nuLQRPPCvvVx80jjrtWmMMT7SW43anyZOnMjMmTMBmDFjBosWLUJEmDlzJnl5eezbt49nn30WgHPOOYfS0lKqqqpYu3Ytzz33HAAXXXQRKSkpALz22mts2LCB+fOdxUXr6+tJT08PwDc7NkvqrrAw4fozJvKj57fx/t4yFhyXGuiQjDHGDFB0dHTH67CwsI7tsLAwWlpa+jxzm6py9dVX88tf/tKncfqKNb93cvmcDEaOiOKBf+0JdCjGGGMGwRlnnNHRfP7mm2+SlpZGYmIiZ555Jo8//jgAL730EuXlzlwmixYt4plnnqGkpASAsrIy9u0bOkOiLal3EhMZzlcXTODVnSV8cqgm0OEYY4zxs9tvv50NGzaQk5PDrbfeyiOPPALAsmXLWLt2LTNmzOC5554jK8uZSnz69On8/Oc/5/zzzycnJ4fzzjuPoiJvfbGWLl3KwoUL2b17NxkZGTz44IM+/z4y3KZHnTdvnq5fv95v5z9c08ipv3qdy+eM55eX5fjtc4wxJpjt3LmTadOmBTqMYelY105ENqjqvN6O9VtNXURWikiJiGzr5v0rRWSLiGwVkXdE5CR/xdIXafHRfGleBs9sKKCwoj7Q4RhjjDGe+bP5/WFgcQ/v7wU+o6ozgZ8BK/wYS59886xJAPz+zdwAR2KMMWY4KS0tZdasWZ96lJaWDsrn+3M99bUikt3D++902nwPyPBXLH01PjmWL87L5M8fFPCtsycxNik20CEZY4wZBlJTU9m8eXPAPn+odJS7Hngp0EF0dtNZx6Mov3/zk0CHYowxxngS8KQuImfjJPUf9FDmRhFZLyLrDx06NChxZaTE8YW5GTy5bj/Flf1f29YYY4wZLAFN6iKSAzwALFHVbm84qOoKVZ2nqvNGjRo1aPHddNYkFOWuVz8atM80xhhj+itgSV1EsoDngK+p6pDMmpkj47h6YTZPrd/v88UIjDHGDB8DXXp1//79nH322UyfPp0ZM2Zw9913+yNMvw5pewJ4FzhBRApE5HoR+YaIfMMtchuQCtwnIptFxH+Dzwfg2+dMJik2kl+s2cFwG9NvjDFmaIiIiODOO+9kx44dvPfee9x7773s2LHD55/jt6SuqktVdayqRqpqhqo+qKr3q+r97vs3qGqKqs5yH70Oqg+EpLhIvrNoMm/nlvLG7pJAh2OMMcajvLw8pk6dyjXXXMOUKVO48sorefXVVznttNOYPHky69ato6ysjEsuuYScnBwWLFjAli1bAGdo2vnnn8+MGTO44YYbPrX06sknn8ysWbP4+te/Tmtra6+xjB07ljlz5gCQkJDAtGnTKCws9Pl3tgVdPPjqggk8+u4+fvHiTs6YPMrWWzfGmL546VYo3urbc46ZCRf8qtdiubm5PP3006xcuZL58+d3LL26evVqli9fTmZmJrNnz+aFF17g9ddf56qrrmLz5s0dS6/edtttvPjiix1TunZeejUyMpKbbrqJVatWdSzZ6kVeXh6bNm3ilFNO6ffX744ldQ8iw8P40UXTuP6R9dz/5id8e9HkQIdkjDHGg6G29GpNTQ2XX345d911F4mJib78qoAldc8WTRvNRTlj+d3ruSw+cQyTRycEOiRjjBkePNSo/WUoLb3a3NzM5ZdfzpVXXslll13W5+O9CO125NrD8OavoNnbOPQ7Lp5BXHQ4//XsFlrbrNOcMcYMd4O19Kqqcv311zNt2jRuueUWP32bUE/qB7fDm7+EjX/yVDwtPppln5/OpvwKHn4nz7+xGWOM8bvBWnr17bff5tFHH+X111/vmA9+zZo1Pv8+ob30qio8dAGU74N/3wSRMR4OUa5/ZD1v5R7muW+eyonjk3wTizHGBBFberX/huTSq8OCCHzmB1B9wHNtXUT47y/kkDoiim+u2kBlXbOfgzTGGGO8Ce2kDnDcWTDhNPjnr6HB26xxqfHR/N9X5lBU0cD3nt5Mm91fN8YYQ+CXXrWkLgLn/xzqDsNb/+P5sLkTUvjxRdN4dWcJd732sR8DNMYYM1y0L7169CM1NXVQPr/XpC4ivxGRRBGJFJHXROSQiHx1MIIbNOPnwElL4d174aD3afuuPjWbL87N4J7XPuaht/f6MUBjjBl+hlufraFgoNfMS039fFWtAj4H5AGTgP8c0KcORef9DKIT4C83QWuLp0NEhF9eNpPzp4/mjr/u4LmNBX4O0hhjhoeYmBhKS0stsfeBqlJaWkpMTO+dtrvjZfKZ9jIXAU+raqWI9PsDh6z4UXDRnfD0NfD2XXDm9z0dFhEexj1LZ3Pdwx/wn88449e/OC/Tv7EaY8wQl5GRQUFBAYcOHQp0KMNKTEwMGRkZ/T7eS1L/m4jsAuqBb4rIKMDbbC3DzYxLYfsLzoQ0J1wIo6d7OiwmMpwVV83j64+u5z+f2cL+8nq+e+5kgvKPH2OM8SAyMpKJEycGOoyQ02vzu6reCpwKzFPVZqAWWNLbcSKyUkRKRGRbN++LiNwjIrkiskVE5vQ1eL+46E6ISYJnr4emWs+HxUdH8NA1J/MF9x77d5/aTE2jt2Z8Y4wxxhe8dJT7ItCsqq0i8mPgMWCch3M/DCzu4f0LgMnu40bg9x7O6X8j0uDyP8KhXbD6284ENR5FRYTx31/I4fvnT2H1hwdYfNda3tszOMMYjDHGGC8d5X6iqtUicjpwLvAgHhKwqq4FynoosgT4kzreA5JFZKyXoP3u+HPgnJ/Atmfhvfv6dKiIcPM5k3n6GwsJDxOW/vE9fvjcVoorg/OOhTHGmKHDS1JvX/39ImCFqr4IRPngs8cD+zttF7j7hobTvwtTPwev/ARyX+3z4XMnjOSl75zBNadm88yG/Zz532/ws7/tYH9ZnR+CNcYYY7wl9UIR+QPwZWCNiER7PM5nRORGEVkvIusHrSelCFx6P6RPhye/Cnlv9fkUcVERLPv8DF7/3lksOWkcD729lzN+8wZf+sO7PLEun/1ldTbcwxhjjM/0uqCLiMTh3Bvfqqofu03kM1X1lV5PLpIN/E1VTzzGe38A3lTVJ9zt3cBZqtrjcjc+XdDFi5pD8PBFUFUIX3seMk/u96kKK+p5YVMhz24oYM9hpxPeuKQYTspMJjttBNmpcaTFRxMfHcGIaGdgQpsqrW1KmyqNLW00NrfR2NLa5XVDp30AsVHhxEWGExcVQWJsJGOSYhibFENafDThYdYj3xhjhhuvC7p4WqVNRE4CznA3/6WqH3oMIpvuk/pFwM3AhcApwD2q2mvGHPSkDlBV5KzmVlcG174IY2YO6HSqyu6D1by/p4z395ayq6ia/LI6WgY4h3z7CLru/pOGhwljEmM4Pj2eSaPimZR+5DFyhC/uqBhjjPEHnyV1EfkO8G/Ac+6uS3Hurf+ul+OeAM4C0oCDwDIgEkBV7xdnEPf/4bQC1AHXqmqv2TogSR2gYj+s/Cy0NsP1f4eRx/n09C2tbRRVNlBW20RNYws1jS0ITiIOEyEsTIiOCHMf4cREhhEdGd6xLyYynAi3Ft7Y0kZdUyt1TS1U1DVTXNlAcVUDxZUN7C+vI7ekhk8O1dDQ3Nbx+WnxUUxOT+CEMQlMHh3PlNEJTElPICku0qff0xhjTN/5MqlvARaqaq27PQJ4V1VzfBJpHwUsqQMc2g0rF0NMIlz3d0gYE5g4fKCtTSmsqCf3UA25B2v4uKSa3QdryD1YTW1Ta0e59IRoJ9GnJ3B8+gjGJ8eSkRLH+ORYYqPCA/gNjDEmdHhN6l5mlBOO9IDHfR2aN2ZHnQBXPgOPfB4euxyueRFikwMdVb+EhQmZI+PIHBnH2Sekd+xva1MOVNbz8cEaPjpYze6D1Xx8sIbH1+3rUrMHSB0RxfiUWNLioxk5IorUEVGkjIjqeD1yRBTJcVGkxEWSEBNp9/ONMcbPvCT1h4D3ReR5d/sSYKX/QhriMubCFY/Bqi/BE1c4neciYwMdlc+EhQkZKXFkpMRx9tSuyb64qoHCinoKy+sprKinoLyOwooGDlY1sLOoitLaJppa2o55XhFIio0kOTaSJDfRJ8dGkhwXRbL7OmVEFOkJTqe+MUkxxERaS4AxxvSF145yc4DT3c1/qeomv0bVg4A2v3e27Tl45lqY+SW4bMWRXmohTFWpbWqlvLaJ0tomymobqahrdh71zVTUNVFR10x5XROV9c0dr6sbjj2dbkpcJKMTnSQ/NjmWrJFxTBgZx4TUEUxIjesYIWCMMcHOl83vqOpGYGOnk+eratYA4hv+TrwMyj6B138O6dPgjFsCHVHAiQjx0RHER0eQOTLO83EtrW1UNbRQVttESVUDRW7HvqLKeoorne1N+yuoqGvuclxafJST6N0kPyE1jqyRztDAkSOibEEdY0zI6W9Vx/61BDjj+1CyE177qZPYT7gg0BENSxHhYYx078FPSo/vtlxlfTP5pXXsK6tlX2ldx+v39pTy/KbCLmXjoyPIGhlHdpqT6CekurX8tBGMTYwhzO7vG2OCUH+Tuk2DBk6T+5J7oWwPPHsDXP8Pz8u1mr5Lio1kZkYSMzOSPvVeQ3MrBeV17Cttf9Syr6yOXUXV/GPHQZpbj/xko8LDyBgZS3bqCLemH+e8To0jIyWW6Ai7l2+MGZ66Teoi0l17sgDdV6dCTWQsXPE4rDgbnvgy3PhPiBsZ6KhCTkxkOJPSE5iUnvCp91rblAMV9eSXdUr4pXXsK6vj/T2lXYbwicC4pFiy05xm/ezU9mfnDwAbxmeMGcp6qql/+l/HI+72dSDDWuI4J7Gv/Cz89d/hS49ax7khJLzT8L3TJnV9T1UprW06kug71fJf3lZMWW1Tl/JjEmM6avYT0txnN/HHW8c9Y0yAeer9PpQMmd7vx/L2PfCPn8DFv4M5VwU6GuMD7ffx95bWsu9wLXlu0s8rreNwTWOXsqMSortMv3u8+3p0YrR12jPGDIhPe78bjxbeDLn/gJd+AFmnQtqk3o8xQ1pP9/FrGls6avh7D9ey93AtuSU1vLCpkOrGI8P0EqIjOO6o+faPH+U050eED+qCh8aYIGc1dV+rOgD3LYSRE+G6VyDCFkoJNarKoepGcktqnGl43bn2c0tqOFh1pHYfFR5GdlocU0YncMLoBKaMcZ4zR8bZ7HvGmC58ukrbUDLkkzrAjr/An6+C02+Bc5cFOhozhFQ1NPNJSQ2fHHJq9bkl1Xx0sIb8srqOMjGRYUxOT2DK6ASmjjmS7K0Z35jQ5bPmdxGJBi4HsjuXV9WfDiTAoDZ9Ccz+Grz1v3D8OTDxjN6PMSEhMSaS2VkpzM5K6bK/trGF3JIadh+s5qNiZ879t3IP8ezGgk7HRnDCGCfZtz9PH5dIYoytpGeMcXhZpe1loBLYQKeFXVT1zl5PLrIYp6d8OPCAqv7qqPezgEeAZLfMraq6pqdzDouaOkBjDfzhTGhpgG++DbEpvR9jzFEq6prYXVzdsbjOR8U17CquoqrT1LrZqXHMGJ/EzPFJnDguiRPHJ5IcZ7d9jAkmvlx6dZuqntiPAMKBj4DzgALgA2Cpqu7oVGYFsElVfy8i04E1qprd03mHTVIHKNwAD54PE06DK1ZBdE+jBI3xRlUpqW5kZ1EV2w9Usa2wkq2FlRSU13eUyRwZ6yZ4J9nPHJ9EyghL9MYMV77s/f6OiMxU1a19jOFkIFdV97gBPQksAXZ0KqNAovs6CTjQx88Y2sbPhc/fA6u/DQ9dAF98BFKPD3RUZpgTEUYnxjA6MYazOi2bW1HXxLbCKrYdcJL8tsJKXtpW3PF+1sg4ZmYkcVJGEjPHJ3Pi+EQSrOnemKDiJamfDlwjInuBRpwZ5VRVc3o5bjywv9N2AXDKUWVuB14RkW8DI4BzvQQ9rMy+EuLTnWlk7z8dzvw+nPJNiPK+4IkxXiTHRXH65DROn5zWsa+yvpnthZVsKaxkS0EFm/MreHFLEeDMj3Rc2ghOykhmZkYSORnJTB2TYKvfGTOMeWl+n3Cs/aq6r5fjvgAsVtUb3O2vAaeo6s2dytzixnCniCwEHgROVNW2o851I3AjQFZW1tx9+3r86KGp6gC8+H3Y/SLEJMGMy2DSIkifDkmZNvTNDJrSmkYnye+vZGthBR8WVHKo+shQu/SEaCamjeh4ZKeNcFfAiyMuyhK+MYHg0yFtInIS0N6F+1+q+qGHYxYCt6vqZ93tHwKo6i87ldmOk/j3u9t7gAWqWtLdeYfVPfVjyX8fPngAdv0NmtuHMQkkjIGoERAR68wnHxkDkXFOLT91EoyfBxnzLfkbn1NVDlY18mFBBbklNR0T6ew9XPupaXLT4qM7VrzLdBfDmZDqvB4Vb0PujPEXX3aU+w7wb8Bz7q5LgRWq+rtejovA6Si3CCjE6Sj3FVXd3qnMS8BTqvqwiEwDXgPGaw9BDfuk3q6lCYo2Q2kulO+DqgJorncfddDcAM21UF0MtYecY6KTYPrnYf4NMG52YOM3IaGyrpk8dy78/WXOFLn5Zc6yt0VVDXT+PzUuKpyskU6NfuqYBKaPS2LGuEQyUmIt2RszQL5M6luAhapa626PAN71cE8dEbkQuAtnuNpKVf2FiPwUWK+qq90e73/EWfVNgf9S1Vd6OmfQJPW+qD0M+e/B7jWw/QUn2U+7GM693TremYBpaG6lsKKe/NK6jhXw8suO1PLb3H9aEmIimD42kenjEpkxLonpYxOZlB5PVIRNkWuMV75M6luB+ara4G7HAB+o6kyfRNpHIZnUO2uogvd+D2/fDa2NcMo34JwfO032xgwRDc2t7CquZseBKnYUVbL9QBW7iqqpb3amuogKD2Py6PiOZD99bCLTbCIdY7rly6R+C3A18Ly76xLgYVW9a8BR9kPIJ/V21QfhjZ/Dxj8599wvuR8y5wc6KmO61dqm5JXWsuOAM75+R1EVOw5UcrjmyH378cmxTBkdzwljEjlhTDwnjE7k+PQRREfYOvYmtPm6o9wcnKFt4HSU2+yMWOoAABvVSURBVDTA+PrNkvpR9rwJf7kZqgrh9O/CWT+EcKvtmOGjpLqB7Qeq2FlUxUfF1ewqruaTQzU0tzr/NoWHCRPTRnDCmASmugvfTB2TQGZKHGG28I0JEQNO6iKSqKpVIjLyWO+ratkAY+wXS+rH0FAFL/8QNj/mdKC77I+QNjnQURnTb82tbeQdrmWXO0XuruJqdhdXd1n4JjYy3K3VJzg1e3dO/FEJ0QGM3Bj/8EVS/5uqfs6ddKZzofbJZ47zTah9Y0m9Bzv+An/9DrQ0wmd/AXOvdWYYMSZI1Da28HFJDbuLqzoS/u7i6i5N+KkjojoWvWlf5W7K6ATibVIdM4zZ0quhquoAvHAT7HkDplwAF/8O4kcFOipj/OpwTWNH0/1ud5W7jw5WU9fUsQYVmSNjO2rzzrK2iRw3agSR4dYL3wx9vuwo95qqLupt32CxpO5BWxus+wP8YxlEx8Nnl0POl63WbkJKW5tSUF7P7oPVXWr2ew7V0uKOt4sMF45La2/Cd9atnzo2gfHJNrbeDC2+aH6PAeKAN4CzcJrdwVmA5WVVneqbUPvm/7d35+F1Vee9x7+vJssaLFuSZ1uewZhgG0MYHAIBSiC0BW7KJUCguU3y0NLQNm3TJ+Gmbbjctkm5JE1zmwkSnpKGQBISbmjaEEiYCgRsAzZgGw8YywO2bEu2Ndka3/vHWkc6Mppsn6MjnfP7PM9+9j5777O19mLj96y116CgfhzqNsC//ynsWg3zPwBXfhmqF2Y6VSIZ1dbZxbb9LX3e1W/a28TuQ72z3E0oLuC02N3utOmhy92iqWVqhS8Zk4qg/mfAp4EZhBHhEkG9EbjX3f8lRWk9Lgrqx6m7C9bcB7++M4xU995PwkWfhZJ+2z+K5Kymox1srmti454mNuwJrfGT+9YX5BkLp4S+9ckBv1JT2soISGX1+58MNSTsSFJQP0FNdfD0P4R+7ePK4bw/hnNuUXAXGURXt1Nb3xL71IdAv2FPI3WNvRPgTJtQHAN8OUumV3Da9HLmVpWqu52kVKr7qb8HWAIUJ/a5+/dOKoUnSEH9JNVtgCf/LswWV1QGZ38czv9UmFBGRIalvrktlugPh/U7jWzd30xXfFdfUpTP4mnlfUr0i6eVa5Y7OWGpLKl/gfBOfQnwn8CHgOfc/doUpPO4KainSN0GeO6f4I2HIa8Qll4H590KU0/PdMpExqSjHV1s3dcch8Zt7KnCbzraCYR2qvOqS3ve0SeGyJ1SrtntZGipHvt9GfCquy8zs6nA9939stQk9fgoqKdYwzZ44f/C2geh8wjMuyhUzS/6IOSpq4/IyXAPLfATAX7DO41s3NvIzobeRnlVpUVJJfpQha+udnKsVAb1Ve5+jpm9DFwMNAEbh9P63cyuAP6ZMEvbd9z9S/2ccx1wB2GAm3XufuNg11RQT5PWBnjlflh1bxhytnI+nHsrLL8xdIsTkZQ5fKSDN/f0vqPfuCf0rW/v7AZ6J7xZPC1U2y+eHkfL05z1OSuVQf0bwP8Ergf+EmgG1rr7HwzxvXzCfOqXAbsI86nf4O4bks5ZBPwIuMTdD5rZFHffN9h1FdTTrKsDNj4aZoLbtTrM4b7i5tCobtKcTKdOJGt1dHWzbX9LUqAPfev3N/U2yqsqLYoj5fUG+0VTyhlfpK522S4tI8qZ2Vxggru/NoxzzwfucPfL4+fbAdz9i0nn3AVsdvfvDDcNCuojaOdqeOmbYQ53HBb/dqian7My0ykTyRkNLe28uTd0r9u0t4k39zayua65p6udGcytKmXxtPI+Ab+mUhPeZJPhBvUBm2LGmdkGPOburwxx7ZnAzqTPu4BzjznnlHi95wlV9He4+2NDXFdGyuz3huWy/w2r74WX/xU2/jvMuxAu/QLMGvL5EpGTVFlaxMoF1axcUN2zr6vb2dHQyqa9seo+DpH72Pq9JMpp4wvzw4x2iXHwp4eAr3712W2wwWeeipvFwNnAOsIANEuBNe5+/qAXNrsWuMLdPxk/3wyc6+63JZ3zc6ADuA6YBTwLnOHuh4651i3ALQA1NTVn1dbWHudtSkq0t4b37s/eDa0H4NTfhkv+GqYuyXTKRARobe9kS10zm/Y2sXFvY0+wb2jpnfBmcvm4UHUfZ7dbPK2chVPKKC5UFf5odtIldXe/OF7op8AKd389fn4PoWHbUHYDs5M+z4r7ku0CXnL3DuBtM9sMLCK8f09Oyz3APRCq34fxtyUdikpCt7czbw7v3F/4GnxzZRhX/uLbYdLcTKdQJKeVFBWwbPZEls2e2LPP3dnf3NYzHG5olNfI935TS1tsmJefZ8ytKumpuk9U48+aNF5V+GPMcBrKrXf304fa18/3CggN5S4lBPPVwI3uvj7pnCsIjec+ZmbVwKvAcnevH+i6eqc+irQ2hL7uq+6B7k5Y8fvw/s9AxcxMp0xEhtDZ1c32+tYY7BvZ2M+c9aVFsQo/TnZzSlxXlWnO+pGWytbvDwItwPfjro8CZe5+wzAScSXwVcL78vvc/e/N7E5C9f2jFvpmfBm4AugC/t7dHxrsmgrqo1DjHvivu+Hl+8HywvjyF/y5pnwVGYOa2zp75qlPNMx7c28Th1o7es6pLhvHqdPK4hS2YSrbRZqzPq1SGdSLgVuBC+OuZ4FvuvvRk07lCVBQH8UO1sIzd8G6H0DBeDj3D+HcP4LyqZlOmYicBHdnf1NbnMY2LJvrmvq0wgeYNWl8T5BPTGc7v7qMogINpHOy0tKlbTRQUB8DDmyBp78Eb/wE8grgjGtDcJ+xPNMpE5EU6u52dh5s7Qny/c1ZX5BnzKsu7WmJn6jCV5e745OKqVd/5O7XxWFi33WSuy89+WQePwX1MaT+LXjp2/Dq96GjBWpWhvfuS66CotJMp05E0qS9s5ttB5p7gv2mvWHEvOThcccX5rNoaqjCP2VqGYumhhL+jIpijZrXj1QE9enuvsfM+h1GzN0z0q9MQX0MOnIoBPbV98LB7WF2uNOvgeU3Qc15YfQMEcl6LfF9fQj0zT2l+wPNvaPmlY0rYOGUMk6JAX9RDPrTJuR2sFf1u4w+7rDjN/DqA7D+kVB6nzQXll4Pyz4SxpsXkZxzsKU9BPt9zWyJQX9LXTP1Sf3ry4sLWDSlb6A/ZWp5zsxyl4qSehP9VLsTBqBxd59wckk8MQrqWaKtOYxO99pDsO0ZwGH2ebDs+lCKHz8p0ykUkQyrb25jc10zW/b1NszbUtfEwaSW+BOKC94V6BdNLcu6yW9UUpex4/BueP1HYfrXA5sgfxyc+iFYdgMs/C3IVzcZEQncnQPN7T0l+t7SfTOHj/QG+4klhZwypbznvX1iXT1G+9inPKib2RTCkLEAuPuOE0/eiVNQz2LusGctrHsIXv8xtNZDRQ2c90dhFLvijFQOicgYkOh2t7kuvKsPpfuw3XS0s+e8ytIiFk0pY9HUMuZWlYaluoRZk0pG9VC5qeynfhVhgJgZwD5gDmE+9UFHlEsXBfUc0dUBm34RhqPd8QIUlcNZHwt93yfWZDp1IjJGuDv7mtr6VN9vrmti675mGpOCvRnMqBjP3OoS5lSVMreqhJrKUmoqS5hTVUJphgfWSWVQXwdcAvzK3c80s4uBm9z9E6lJ6vFRUM9Bu1+BF78Bb/wUcDjtqjAF7Oxz1HJeRE7YwZZ2tte3UFvf2me9/UBLn/f2ANVlRcyuLGFOZQk1VaVxHT5PHoHGeqkM6mvc/ewY3M90924zW+fuy1KV2OOhoJ7DDu8K/d5fuR+OHoYZZ4bgvuQaKNB0kiKSOoePdLCzoZXa+lZqG1p6t+tb2XP4CN1JobO4MI+aylCyn1MVSva/t2JWSkv3qQzqvwKuAb4IVBOq4N/r7itTkdDjpaAutLfAugfhxW9B/RYonQzvuTa0nJ++TKV3EUmr9s5udh86Qm19CzsaWtlR30ptXO9oaOVIRxcb7ryckqLRGdRLgaOErmwfBSqABwabSS2dFNSlR3c3vPVkKLlvfgy62mHy4jAV7NLroGJWplMoIjkm0Tp/cnlqW9mnop/614EfuPvzKU3ZSVJQl361NsCG/xdazu98CbAwWt2Sa8KwtBNmZDqFIiInbLhBfbCpczYDd5vZdjO7y8zOPIFEXGFmm8xsq5l9bpDzfs/M3MyGTLBIv0oq4eyPwycehz99FT5we3jv/thn4SunwXcvh998I/SJFxHJUsOpfp8DXB+X8cCDwIPuvnmI7+UTfhhcBuwCVgM3uPuGY84rB/4DKAJuc/dBi+Eqqctx2b8ZNvwslOLr3gj7Zp0TRq1bcrWq6EVkTEjLiHKxtH4fsNTdB+2lb2bnA3e4++Xx8+0A7v7FY877KvAE8FfAZxTUJW0ObIUNj8D6n0Hd62HfrPf2VtGr/7uIjFKpqH5PXKjAzH7XzB4AfgFsAj48jDTMBHYmfd4V9yVfewUw293/YxjXEzk51Qvhwr+CW5+DP3kFLv1b6GyDxz8PXz0D7r0Env8aHMzIBIQiIidtwPb2ZnYZcANwJbAKeAi4xd1bUvGHzSwP+ArwP4Zx7i3ALQA1NSpNSQpULYD3/2VY6t/qraJ/4m/CUn0qLLwUFlwCc1Zq/ncRGRMGa/3+JPAD4CfufvC4LzxE9buZVQBvAc3xK9OABuCqwargVf0uadXwNrz589BVrvYF6DwK+UWhJf28i2DO+2DmCigYm5NCiMjYlPFZ2sysgNBQ7lJgN6Gh3I3uvn6A859G79RlNOk4EuZ/f+tJ2Pok7IuPbkExzDw7lODnrAzD1aokLyJpNNygnrYR6t2908xuA34J5AP3uft6M7sTWOPuj6brb4ukROH4UP2+4BL4INBSH4J87QtQ+zz8193wbDfkFcD05TD3faEkP/tcGD8x06kXkRyk+dRFTtTRRti1Kgb5F2D3y2FUOwymvScE+DkroWYllE3OdGpFZAzLePV7uiioy6jVcSQE9kRJfucq6GgNx6pPidX1MdCrf7yIHIeMV7+L5JzC8TD3grBAmBN+z7oQ4GtfgDcegZf/NRybWBMCfM35IchXLdRENCJy0lRSFxkp3V2wb0NvSb72BWjZH46VVMHs86Dm3LCesVwt7EWkh0rqIqNNXj5MOyMs5/4huMOBLbDzRdjxUmiEtymOw5Q/LnSdqzkvBPnZ54Tx7UVEBqGSusho0rwvzDK348Ww7FkL3Z3h2OTFoWV9zfmhRD9pnqrsRXKEGsqJZIP2Vnjnld4gv3MVtB0Ox0qnwORTw+h4lQvien4I9oXFmU23iKSUqt9FskFRSd/Gd93dsH9jCPC71kD9FtjwKBxpSPqShdb1lfPDhDULLg5rvaMXyXoqqYtkgyMHoWEb1G+DhrfCePYHNsPe18G7oLAktLKfdxHMuxCmLYW8IedzEpFRQiV1kVwyfhLMPCssyY4ehu3Pw7anw/LE34T9xRNh+lKYegZMXRKq7yvnQ9kUvacXGcMU1EWyWXEFLL4yLACNe+DtZ0OXur2vw5rvhklrEgpLoXJeXOL7+cr5YamYpYAvMsopqIvkkgnTYdlHwgLQ1QmHasPsdAffDlX4Ddtg/ybY/Ms47G00flJ4N59YZp4FxRMycx8i0i8FdZFcll8QWs1XLXj3se4uaHwnvqvfCu+8CrtWw5bH4wkWv7swLJPmhtJ8xSyomB1qCVSyFxlRCuoi0r+8fJg4OyzzL+rdf+RQ6Ga3czXUvR4a5217um81PkBReRgOt3pR6HpXfUpcFoUhdUUk5dIa1M3sCuCfCVOvfsfdv3TM8b8APgl0AvuBj7t7bTrTJCInafzE3ilpE7q7w5C3h3fB4Z2964PbYe9rsPFR8O54ssVgf0rfYF+1EEqrVboXOQlpC+pmlg98HbgM2AWsNrNH3X1D0mmvAme7e6uZ3QrcBXwkXWkSkTTJy4PyqWGZdda7j3ccDV3tDmyG/ZvhwKawvf056DzSe15BMUyYARNmhir8ilmhWj/RcK9sqoK+yCDSWVI/B9jq7tsAzOwh4GqgJ6i7+1NJ578I3JTG9IhIphQWw9TTw5KsuxsO7whj4DdsiyX8XdC4G95+Bpr2JJXwCf3tJ83rbaE/aV4I/hNmhGX8JAV9yWnpDOozgZ1Jn3cB5w5y/ieAX6QxPSIy2uTlhZL4pLn9H+/qgEM7Ysv8xBIb7m15Arra+p5fML43wPcsM8O6fHrYLp2sgXcka42KhnJmdhNwNnDRAMdvAW4BqKmpGcGUiUhG5RcO0jq/O5TkG3fHJbH9TlhqfxOOd3f0/V5eAZQnAn4M9OXToXxa77psKowrG5l7FEmhdAb13cDspM+z4r4+zOy3gM8DF7l727HHAdz9HuAeCMPEpj6pIjLm5OVBxcywDKS7G1oP9A32Pctu2PMabHqs73v9hKLy0EagLC6JYF8+LYy8Vzo5LCVV4ceHyCiQzqC+GlhkZvMIwfx64MbkE8zsTODbwBXuvi+NaRGRXJSXFwJw2RSYcWb/57jD0UPQVBdK9s1x3VQHzXvDes9a2FwHHS39X6O4Igb46tCCv7Q6aTsG/uRt/QiQNElbUHf3TjO7DfgloUvbfe6+3szuBNa4+6PA/wHKgB9baNyyw92vSleaRETexSw0sBs/CaYsHvzctqbeYN9yINQCtNSH7nytB8K++rdg50vQWt+3kV+y4oreoF88MXQT7FlX9LMvrgtL1BBQBqVZ2kRE0qG7O9QAtOxP+gGwP/wISGy31odJd44cCucebQQG+Tc5r7D/YD/UetwEKCpTA8ExTLO0iYhkUl4elFSGZfKpw/tOdxe0NfYG+aHWLfuhfkv4YXD08MA1AwlFZWEZl1iXhyWxb1x5aEuQfLyoDIpKQi1BYUnvdlEp5Bep5mCUUVAXERkt8vJ7XwUcr+5uaG/q/wdAWyO0NUN78zHbzXBoZ/heYt+xw/0OxvJDcO8J9qUD/wBIPqewOHQ/HO5aNQzDpqAuIpIN8vLi+/gKYM6JX6erI7QdaG+O69bQQLC9FTpaob3lmHU/x9uboXnfu/cP9mphMPlFSUG+OMwdMJx1wbjQKDF/HBQUxfW4eL3kY8n7ivpuF4wL54yRHxYK6iIi0iu/sPe1QSq5h1qAjiNhSWwf17o1DDnceSSuj4YfHi37331u59GhX0ccj7yCENzzC3sDfc8PhaJ3H/vwPRkZ60BBXURE0s8slKBHcoa+rs4w6mBXO3S2h+2eddzf51ji3ORjbf2v37UvXru9JezLUFsDBXUREclO+QVhoTTTKRkxY+MlgYiIiAxJQV1ERCRLKKiLiIhkCQV1ERGRLKGgLiIikiXG3NjvZrYfqE3hJauBAym83lin/OhL+dGX8qMv5Udfyo++Upkfc9x98lAnjbmgnmpmtmY4g+TnCuVHX8qPvpQffSk/+lJ+9JWJ/FD1u4iISJZQUBcREckSCupwT6YTMMooP/pSfvSl/OhL+dGX8qOvEc+PnH+nLiIiki1UUhcREckSOR3UzewKM9tkZlvN7HOZTk8mmNl2M3vdzNaa2Zq4r9LMnjCzLXE9KdPpTBczu8/M9pnZG0n7+r1/C74Wn5fXzGxF5lKeHgPkxx1mtjs+I2vN7MqkY7fH/NhkZpdnJtXpYWazzewpM9tgZuvN7M/i/px8PgbJj1x9PorNbJWZrYv58b/i/nlm9lK87x+aWVHcPy5+3hqPz01Lwtw9JxcgH3gLmA8UAeuAJZlOVwbyYTtQfcy+u4DPxe3PAf+Y6XSm8f4vBFYAbwx1/8CVwC8AA84DXsp0+kcoP+4APtPPuUvi/zfjgHnx/6f8TN9DCvNiOrAibpcDm+M95+TzMUh+5OrzYUBZ3C4EXor/3X8EXB/3fwu4NW7/MfCtuH098MN0pCuXS+rnAFvdfZu7twMPAVdnOE2jxdXA/XH7fuCaDKYlrdz9WaDhmN0D3f/VwPc8eBGYaGbTRyalI2OA/BjI1cBD7t7m7m8DWwn/X2UFd9/j7q/E7SZgIzCTHH0+BsmPgWT78+Hu3hw/FsbFgUuAh+P+Y5+PxHPzMHCpWeonXc/loD4T2Jn0eReDP6DZyoHHzexlM7sl7pvq7nvi9l5gamaSljED3X8uPzO3xSrl+5Jex+RMfsSq0jMJpbGcfz6OyQ/I0efDzPLNbC2wD3iCUBtxyN074ynJ99yTH/H4YaAq1WnK5aAuwQXuvgL4EPApM7sw+aCHuqKc7SKR6/cffRNYACwH9gBfzmxyRpaZlQE/AT7t7o3Jx3Lx+egnP3L2+XD3LndfDswi1EIsznCScjqo7wZmJ32eFfflFHffHdf7gEcID2ZdotowrvdlLoUZMdD95+Qz4+518R+vbuBeeqtQsz4/zKyQEMAecPefxt05+3z0lx+5/HwkuPsh4CngfMJrl4J4KPmee/IjHq8A6lOdllwO6quBRbGlYhGh4cKjGU7TiDKzUjMrT2wDHwTeIOTDx+JpHwN+lpkUZsxA9/8o8PuxlfN5wOGkatisdcx74f9GeEYg5Mf1sVXvPGARsGqk05cu8X3nd4GN7v6VpEM5+XwMlB85/HxMNrOJcXs8cBmhncFTwLXxtGOfj8Rzcy3wZKzpSa1MtyDM5EJorbqZ8B7k85lOTwbufz6hdeo6YH0iDwjveX4NbAF+BVRmOq1pzIMHCVWGHYT3X58Y6P4JrV2/Hp+X14GzM53+EcqPf4v3+xrhH6bpSed/PubHJuBDmU5/ivPiAkLV+mvA2rhcmavPxyD5kavPx1Lg1XjfbwB/G/fPJ/x42Qr8GBgX9xfHz1vj8fnpSJdGlBMREckSuVz9LiIiklUU1EVERLKEgrqIiEiWUFAXERHJEgrqIiIiWUJBXUROipl9wMx+nul0iIiCuoiISNZQUBfJEWZ2U5z/ea2ZfTtORtFsZv8U54P+tZlNjucuN7MX4yQdjyTNGb7QzH4V55B+xcwWxMuXmdnDZvammT2QmH3KzL4U599+zczuztCti+QMBXWRHGBmpwEfAd7nYQKKLuCjQCmwxt1PB54BvhC/8j3gs+6+lDBaWGL/A8DX3X0ZsJIw+hyEGbs+TZhDez7wPjOrIgwbenq8zt+l9y5FREFdJDdcCpwFrI5TRV5KCL7dwA/jOd8HLjCzCmCiuz8T998PXBjnCZjp7o8AuPtRd2+N56xy910eJvVYC8wlTC15FPiumX0YSJwrImmioC6SGwy4392Xx+VUd7+jn/NOdNzotqTtLqDAw5zR5wAPA78DPHaC1xaRYVJQF8kNvwauNbMpAGZWaWZzCP8GJGaUuhF4zt0PAwfN7P1x/83AM+7eBOwys2viNcaZWclAfzDOu13h7v8J/DmwLB03JiK9CoY+RUTGOnffYGZ/DTxuZnmEWdg+BbQA58Rj+wjv3SFMEfmtGLS3AX8Q998MfNvM7ozX+O+D/Nly4GdmVkyoKfiLFN+WiBxDs7SJ5DAza3b3skynQ0RSQ9XvIiIiWUIldRERkSyhkrqIiEiWUFAXERHJEgrqIiIiWUJBXUREJEsoqIuIiGQJBXUREZEs8f8B5TIsm/iOPIgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize = (8, 6))\n",
    "\n",
    "for history in history_dict:\n",
    "    val_acc = history_dict[history].history['val_acc']\n",
    "    val_loss = history_dict[history].history['val_loss']\n",
    "    ax1.plot(val_acc, label=history)\n",
    "    ax2.plot(val_loss, label=history)\n",
    "    \n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 66us/step\n",
      "\n",
      "Accuracy: 97.92%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(train_x, train_y)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 45us/step\n",
      "model_ 1\n",
      "Test Loss : 0.5699\n",
      "Test Acc  : 0.9333\n",
      "30/30 [==============================] - 0s 60us/step\n",
      "model_ 2\n",
      "Test Loss : 0.0843\n",
      "Test Acc  : 0.9667\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for model in models:\n",
    "    score = model.evaluate(test_x, test_y, verbose=1)\n",
    "    print('model_',i)\n",
    "    print('Test Loss : {:.4f}'.format(score[0]))\n",
    "    print('Test Acc  : {:.4f}'.format(score[1]))\n",
    "    i +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
